use crate::Control;
/// Generated by `cargo run --bin generate_c`
use std::convert::Infallible;

pub enum ControlId {
    /// Enable or disable the AE.
    ///
    /// \sa ExposureTime AnalogueGain
    AeEnable = 1,
    /// Report the lock status of a running AE algorithm.
    ///
    /// If the AE algorithm is locked the value shall be set to true, if it's
    /// converging it shall be set to false. If the AE algorithm is not
    /// running the control shall not be present in the metadata control list.
    ///
    /// \sa AeEnable
    AeLocked = 2,
    /// Specify a metering mode for the AE algorithm to use. The metering
    /// modes determine which parts of the image are used to determine the
    /// scene brightness. Metering modes may be platform specific and not
    /// all metering modes may be supported.
    AeMeteringMode = 3,
    /// Specify a constraint mode for the AE algorithm to use. These determine
    /// how the measured scene brightness is adjusted to reach the desired
    /// target exposure. Constraint modes may be platform specific, and not
    /// all constraint modes may be supported.
    AeConstraintMode = 4,
    /// Specify an exposure mode for the AE algorithm to use. These specify
    /// how the desired total exposure is divided between the shutter time
    /// and the sensor's analogue gain. The exposure modes are platform
    /// specific, and not all exposure modes may be supported.
    AeExposureMode = 5,
    /// Specify an Exposure Value (EV) parameter. The EV parameter will only be
    /// applied if the AE algorithm is currently enabled.
    ///
    /// By convention EV adjusts the exposure as log2. For example
    /// EV = [-2, -1, 0.5, 0, 0.5, 1, 2] results in an exposure adjustment
    /// of [1/4x, 1/2x, 1/sqrt(2)x, 1x, sqrt(2)x, 2x, 4x].
    ///
    /// \sa AeEnable
    ExposureValue = 6,
    /// Exposure time (shutter speed) for the frame applied in the sensor
    /// device. This value is specified in micro-seconds.
    ///
    /// Setting this value means that it is now fixed and the AE algorithm may
    /// not change it. Setting it back to zero returns it to the control of the
    /// AE algorithm.
    ///
    /// \sa AnalogueGain AeEnable
    ///
    /// \todo Document the interactions between AeEnable and setting a fixed
    /// value for this control. Consider interactions with other AE features,
    /// such as aperture and aperture/shutter priority mode, and decide if
    /// control of which features should be automatically adjusted shouldn't
    /// better be handled through a separate AE mode control.
    ExposureTime = 7,
    /// Analogue gain value applied in the sensor device.
    /// The value of the control specifies the gain multiplier applied to all
    /// colour channels. This value cannot be lower than 1.0.
    ///
    /// Setting this value means that it is now fixed and the AE algorithm may
    /// not change it. Setting it back to zero returns it to the control of the
    /// AE algorithm.
    ///
    /// \sa ExposureTime AeEnable
    ///
    /// \todo Document the interactions between AeEnable and setting a fixed
    /// value for this control. Consider interactions with other AE features,
    /// such as aperture and aperture/shutter priority mode, and decide if
    /// control of which features should be automatically adjusted shouldn't
    /// better be handled through a separate AE mode control.
    AnalogueGain = 8,
    /// Specify a fixed brightness parameter. Positive values (up to 1.0)
    /// produce brighter images; negative values (up to -1.0) produce darker
    /// images and 0.0 leaves pixels unchanged.
    Brightness = 9,
    /// Specify a fixed contrast parameter. Normal contrast is given by the
    /// value 1.0; larger values produce images with more contrast.
    Contrast = 10,
    /// Report an estimate of the current illuminance level in lux. The Lux
    /// control can only be returned in metadata.
    Lux = 11,
    /// Enable or disable the AWB.
    ///
    /// \sa ColourGains
    AwbEnable = 12,
    /// Specify the range of illuminants to use for the AWB algorithm. The modes
    /// supported are platform specific, and not all modes may be supported.
    AwbMode = 13,
    /// Report the lock status of a running AWB algorithm.
    ///
    /// If the AWB algorithm is locked the value shall be set to true, if it's
    /// converging it shall be set to false. If the AWB algorithm is not
    /// running the control shall not be present in the metadata control list.
    ///
    /// \sa AwbEnable
    AwbLocked = 14,
    /// Pair of gain values for the Red and Blue colour channels, in that
    /// order. ColourGains can only be applied in a Request when the AWB is
    /// disabled.
    ///
    /// \sa AwbEnable
    ColourGains = 15,
    /// Report the current estimate of the colour temperature, in kelvin, for this frame. The ColourTemperature control can only be returned in metadata.
    ColourTemperature = 16,
    /// Specify a fixed saturation parameter. Normal saturation is given by
    /// the value 1.0; larger values produce more saturated colours; 0.0
    /// produces a greyscale image.
    Saturation = 17,
    /// Reports the sensor black levels used for processing a frame, in the
    /// order R, Gr, Gb, B. These values are returned as numbers out of a 16-bit
    /// pixel range (as if pixels ranged from 0 to 65535). The SensorBlackLevels
    /// control can only be returned in metadata.
    SensorBlackLevels = 18,
    /// A value of 0.0 means no sharpening. The minimum value means
    /// minimal sharpening, and shall be 0.0 unless the camera can't
    /// disable sharpening completely. The default value shall give a
    /// "reasonable" level of sharpening, suitable for most use cases.
    /// The maximum value may apply extremely high levels of sharpening,
    /// higher than anyone could reasonably want. Negative values are
    /// not allowed. Note also that sharpening is not applied to raw
    /// streams.
    Sharpness = 19,
    /// Reports a Figure of Merit (FoM) to indicate how in-focus the frame is.
    /// A larger FocusFoM value indicates a more in-focus frame. This control
    /// depends on the IPA to gather ISP statistics from the defined focus
    /// region, and combine them in a suitable way to generate a FocusFoM value.
    /// In this respect, it is not necessarily aimed at providing a way to
    /// implement a focus algorithm by the application, rather an indication of
    /// how in-focus a frame is.
    FocusFoM = 20,
    /// The 3x3 matrix that converts camera RGB to sRGB within the
    /// imaging pipeline. This should describe the matrix that is used
    /// after pixels have been white-balanced, but before any gamma
    /// transformation. The 3x3 matrix is stored in conventional reading
    /// order in an array of 9 floating point values.
    ColourCorrectionMatrix = 21,
    /// Sets the image portion that will be scaled to form the whole of
    /// the final output image. The (x,y) location of this rectangle is
    /// relative to the PixelArrayActiveAreas that is being used. The units
    /// remain native sensor pixels, even if the sensor is being used in
    /// a binning or skipping mode.
    ///
    /// This control is only present when the pipeline supports scaling. Its
    /// maximum valid value is given by the properties::ScalerCropMaximum
    /// property, and the two can be used to implement digital zoom.
    ScalerCrop = 22,
    /// Digital gain value applied during the processing steps applied
    /// to the image as captured from the sensor.
    ///
    /// The global digital gain factor is applied to all the colour channels
    /// of the RAW image. Different pipeline models are free to
    /// specify how the global gain factor applies to each separate
    /// channel.
    ///
    /// If an imaging pipeline applies digital gain in distinct
    /// processing steps, this value indicates their total sum.
    /// Pipelines are free to decide how to adjust each processing
    /// step to respect the received gain factor and shall report
    /// their total value in the request metadata.
    DigitalGain = 23,
    /// The instantaneous frame duration from start of frame exposure to start
    /// of next exposure, expressed in microseconds. This control is meant to
    /// be returned in metadata.
    FrameDuration = 24,
    /// The minimum and maximum (in that order) frame duration,
    /// expressed in microseconds.
    ///
    /// When provided by applications, the control specifies the sensor frame
    /// duration interval the pipeline has to use. This limits the largest
    /// exposure time the sensor can use. For example, if a maximum frame
    /// duration of 33ms is requested (corresponding to 30 frames per second),
    /// the sensor will not be able to raise the exposure time above 33ms.
    /// A fixed frame duration is achieved by setting the minimum and maximum
    /// values to be the same. Setting both values to 0 reverts to using the
    /// IPA provided defaults.
    ///
    /// The maximum frame duration provides the absolute limit to the shutter
    /// speed computed by the AE algorithm and it overrides any exposure mode
    /// setting specified with controls::AeExposureMode. Similarly, when a
    /// manual exposure time is set through controls::ExposureTime, it also
    /// gets clipped to the limits set by this control. When reported in
    /// metadata, the control expresses the minimum and maximum frame
    /// durations used after being clipped to the sensor provided frame
    /// duration limits.
    ///
    /// \sa AeExposureMode
    /// \sa ExposureTime
    ///
    /// \todo Define how to calculate the capture frame rate by
    /// defining controls to report additional delays introduced by
    /// the capture pipeline or post-processing stages (ie JPEG
    /// conversion, frame scaling).
    ///
    /// \todo Provide an explicit definition of default control values, for
    /// this and all other controls.
    FrameDurationLimits = 25,
    /// Temperature measure from the camera sensor in Celsius. This is typically
    /// obtained by a thermal sensor present on-die or in the camera module. The
    /// range of reported temperatures is device dependent.
    ///
    /// The SensorTemperature control will only be returned in metadata if a
    /// themal sensor is present.
    SensorTemperature = 26,
    /// The time when the first row of the image sensor active array is exposed.
    ///
    /// The timestamp, expressed in nanoseconds, represents a monotonically
    /// increasing counter since the system boot time, as defined by the
    /// Linux-specific CLOCK_BOOTTIME clock id.
    ///
    /// The SensorTimestamp control can only be returned in metadata.
    ///
    /// \todo Define how the sensor timestamp has to be used in the reprocessing
    /// use case.
    SensorTimestamp = 27,
    /// Control to set the mode of the AF (autofocus) algorithm.
    ///
    /// An implementation may choose not to implement all the modes.
    AfMode = 28,
    /// Control to set the range of focus distances that is scanned. An
    /// implementation may choose not to implement all the options here.
    AfRange = 29,
    /// Control that determines whether the AF algorithm is to move the lens
    /// as quickly as possible or more steadily. For example, during video
    /// recording it may be desirable not to move the lens too abruptly, but
    /// when in a preview mode (waiting for a still capture) it may be
    /// helpful to move the lens as quickly as is reasonably possible.
    AfSpeed = 30,
    /// Instruct the AF algorithm how it should decide which parts of the image
    /// should be used to measure focus.
    AfMetering = 31,
    /// Sets the focus windows used by the AF algorithm when AfMetering is set
    /// to AfMeteringWindows. The units used are pixels within the rectangle
    /// returned by the ScalerCropMaximum property.
    ///
    /// In order to be activated, a rectangle must be programmed with non-zero
    /// width and height. Internally, these rectangles are intersected with the
    /// ScalerCropMaximum rectangle. If the window becomes empty after this
    /// operation, then the window is ignored. If all the windows end up being
    /// ignored, then the behaviour is platform dependent.
    ///
    /// On platforms that support the ScalerCrop control (for implementing
    /// digital zoom, for example), no automatic recalculation or adjustment of
    /// AF windows is performed internally if the ScalerCrop is changed. If any
    /// window lies outside the output image after the scaler crop has been
    /// applied, it is up to the application to recalculate them.
    ///
    /// The details of how the windows are used are platform dependent. We note
    /// that when there is more than one AF window, a typical implementation
    /// might find the optimal focus position for each one and finally select
    /// the window where the focal distance for the objects shown in that part
    /// of the image are closest to the camera.
    AfWindows = 32,
    /// This control starts an autofocus scan when AfMode is set to AfModeAuto,
    /// and can also be used to terminate a scan early.
    ///
    /// It is ignored if AfMode is set to AfModeManual or AfModeContinuous.
    AfTrigger = 33,
    /// This control has no effect except when in continuous autofocus mode
    /// (AfModeContinuous). It can be used to pause any lens movements while
    /// (for example) images are captured. The algorithm remains inactive
    /// until it is instructed to resume.
    AfPause = 34,
    /// Acts as a control to instruct the lens to move to a particular position
    /// and also reports back the position of the lens for each frame.
    ///
    /// The LensPosition control is ignored unless the AfMode is set to
    /// AfModeManual, though the value is reported back unconditionally in all
    /// modes.
    ///
    /// The units are a reciprocal distance scale like dioptres but normalised
    /// for the hyperfocal distance. That is, for a lens with hyperfocal
    /// distance H, and setting it to a focal distance D, the lens position LP,
    /// which is generally a non-integer, is given by
    ///
    /// \f$LP = \frac{H}{D}\f$
    ///
    /// For example:
    ///
    /// 0 moves the lens to infinity.
    /// 0.5 moves the lens to twice the hyperfocal distance.
    /// 1 moves the lens to the hyperfocal position.
    /// And larger values will focus the lens ever closer.
    ///
    /// \todo Define a property to report the Hyperforcal distance of calibrated
    /// lenses.
    ///
    /// \todo Define a property to report the maximum and minimum positions of
    /// this lens. The minimum value will often be zero (meaning infinity).
    LensPosition = 35,
    /// Reports the current state of the AF algorithm in conjunction with the
    /// reported AfMode value and (in continuous AF mode) the AfPauseState
    /// value. The possible state changes are described below, though we note
    /// the following state transitions that occur when the AfMode is changed.
    ///
    /// If the AfMode is set to AfModeManual, then the AfState will always
    /// report AfStateIdle (even if the lens is subsequently moved). Changing to
    /// the AfModeManual state does not initiate any lens movement.
    ///
    /// If the AfMode is set to AfModeAuto then the AfState will report
    /// AfStateIdle. However, if AfModeAuto and AfTriggerStart are sent together
    /// then AfState will omit AfStateIdle and move straight to AfStateScanning
    /// (and start a scan).
    ///
    /// If the AfMode is set to AfModeContinuous then the AfState will initially
    /// report AfStateScanning.
    AfState = 36,
    /// Only applicable in continuous (AfModeContinuous) mode, this reports
    /// whether the algorithm is currently running, paused or pausing (that is,
    /// will pause as soon as any in-progress scan completes).
    ///
    /// Any change to AfMode will cause AfPauseStateRunning to be reported.
    AfPauseState = 37,
    /// Control for AE metering trigger. Currently identical to
    /// ANDROID_CONTROL_AE_PRECAPTURE_TRIGGER.
    ///
    /// Whether the camera device will trigger a precapture metering sequence
    /// when it processes this request.
    AePrecaptureTrigger = 38,
    /// Control to select the noise reduction algorithm mode. Currently
    /// identical to ANDROID_NOISE_REDUCTION_MODE.
    ///
    ///  Mode of operation for the noise reduction algorithm.
    NoiseReductionMode = 39,
    /// Control to select the color correction aberration mode. Currently
    /// identical to ANDROID_COLOR_CORRECTION_ABERRATION_MODE.
    ///
    ///  Mode of operation for the chromatic aberration correction algorithm.
    ColorCorrectionAberrationMode = 40,
    /// Control to report the current AE algorithm state. Currently identical to
    /// ANDROID_CONTROL_AE_STATE.
    ///
    ///  Current state of the AE algorithm.
    AeState = 41,
    /// Control to report the current AWB algorithm state. Currently identical
    /// to ANDROID_CONTROL_AWB_STATE.
    ///
    ///  Current state of the AWB algorithm.
    AwbState = 42,
    /// Control to report the time between the start of exposure of the first
    /// row and the start of exposure of the last row. Currently identical to
    /// ANDROID_SENSOR_ROLLING_SHUTTER_SKEW
    SensorRollingShutterSkew = 43,
    /// Control to report if the lens shading map is available. Currently
    /// identical to ANDROID_STATISTICS_LENS_SHADING_MAP_MODE.
    LensShadingMapMode = 44,
    /// Control to report the detected scene light frequency. Currently
    /// identical to ANDROID_STATISTICS_SCENE_FLICKER.
    SceneFlicker = 45,
    /// Specifies the number of pipeline stages the frame went through from when
    /// it was exposed to when the final completed result was available to the
    /// framework. Always less than or equal to PipelineMaxDepth. Currently
    /// identical to ANDROID_REQUEST_PIPELINE_DEPTH.
    ///
    /// The typical value for this control is 3 as a frame is first exposed,
    /// captured and then processed in a single pass through the ISP. Any
    /// additional processing step performed after the ISP pass (in example face
    /// detection, additional format conversions etc) count as an additional
    /// pipeline stage.
    PipelineDepth = 46,
    /// The maximum number of frames that can occur after a request (different
    /// than the previous) has been submitted, and before the result's state
    /// becomes synchronized. A value of -1 indicates unknown latency, and 0
    /// indicates per-frame control. Currently identical to
    /// ANDROID_SYNC_MAX_LATENCY.
    MaxLatency = 47,
    /// Control to select the test pattern mode. Currently identical to
    /// ANDROID_SENSOR_TEST_PATTERN_MODE.
    TestPatternMode = 48,
}

/// Enable or disable the AE.
///
/// \sa ExposureTime AnalogueGain
pub struct AeEnable(pub bool);

impl TryFrom<bool> for AeEnable {
    type Error = Infallible;

    fn try_from(value: bool) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<bool> for AeEnable {
    fn into(self) -> bool {
        self.0
    }
}

impl Control for AeEnable {
    type T = bool;
}

/// Report the lock status of a running AE algorithm.
///
/// If the AE algorithm is locked the value shall be set to true, if it's
/// converging it shall be set to false. If the AE algorithm is not
/// running the control shall not be present in the metadata control list.
///
/// \sa AeEnable
pub struct AeLocked(pub bool);

impl TryFrom<bool> for AeLocked {
    type Error = Infallible;

    fn try_from(value: bool) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<bool> for AeLocked {
    fn into(self) -> bool {
        self.0
    }
}

impl Control for AeLocked {
    type T = bool;
}

/// Specify a metering mode for the AE algorithm to use. The metering
/// modes determine which parts of the image are used to determine the
/// scene brightness. Metering modes may be platform specific and not
/// all metering modes may be supported.
pub enum AeMeteringMode {
    /// Centre-weighted metering mode.
    MeteringCentreWeighted = 0,
    /// Spot metering mode.
    MeteringSpot = 1,
    /// Matrix metering mode.
    MeteringMatrix = 2,
    /// Custom metering mode.
    MeteringCustom = 3,
}

impl TryFrom<i32> for AeMeteringMode {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::MeteringCentreWeighted),
            1 => Ok(Self::MeteringSpot),
            2 => Ok(Self::MeteringMatrix),
            3 => Ok(Self::MeteringCustom),
            _ => Err(()),
        }
    }
}

impl Into<i32> for AeMeteringMode {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for AeMeteringMode {
    type T = i32;
}

/// Specify a constraint mode for the AE algorithm to use. These determine
/// how the measured scene brightness is adjusted to reach the desired
/// target exposure. Constraint modes may be platform specific, and not
/// all constraint modes may be supported.
pub enum AeConstraintMode {
    /// Default constraint mode. This mode aims to balance the exposure of different parts of the image so as to reach a reasonable average level. However, highlights in the image may appear over-exposed and lowlights may appear under-exposed.
    ConstraintNormal = 0,
    /// Highlight constraint mode. This mode adjusts the exposure levels in order to try and avoid over-exposing the brightest parts (highlights) of an image. Other non-highlight parts of the image may appear under-exposed.
    ConstraintHighlight = 1,
    /// Shadows constraint mode. This mode adjusts the exposure levels in order to try and avoid under-exposing the dark parts (shadows) of an image. Other normally exposed parts of the image may appear over-exposed.
    ConstraintShadows = 2,
    /// Custom constraint mode.
    ConstraintCustom = 3,
}

impl TryFrom<i32> for AeConstraintMode {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::ConstraintNormal),
            1 => Ok(Self::ConstraintHighlight),
            2 => Ok(Self::ConstraintShadows),
            3 => Ok(Self::ConstraintCustom),
            _ => Err(()),
        }
    }
}

impl Into<i32> for AeConstraintMode {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for AeConstraintMode {
    type T = i32;
}

/// Specify an exposure mode for the AE algorithm to use. These specify
/// how the desired total exposure is divided between the shutter time
/// and the sensor's analogue gain. The exposure modes are platform
/// specific, and not all exposure modes may be supported.
pub enum AeExposureMode {
    /// Default exposure mode.
    ExposureNormal = 0,
    /// Exposure mode allowing only short exposure times.
    ExposureShort = 1,
    /// Exposure mode allowing long exposure times.
    ExposureLong = 2,
    /// Custom exposure mode.
    ExposureCustom = 3,
}

impl TryFrom<i32> for AeExposureMode {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::ExposureNormal),
            1 => Ok(Self::ExposureShort),
            2 => Ok(Self::ExposureLong),
            3 => Ok(Self::ExposureCustom),
            _ => Err(()),
        }
    }
}

impl Into<i32> for AeExposureMode {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for AeExposureMode {
    type T = i32;
}

/// Specify an Exposure Value (EV) parameter. The EV parameter will only be
/// applied if the AE algorithm is currently enabled.
///
/// By convention EV adjusts the exposure as log2. For example
/// EV = [-2, -1, 0.5, 0, 0.5, 1, 2] results in an exposure adjustment
/// of [1/4x, 1/2x, 1/sqrt(2)x, 1x, sqrt(2)x, 2x, 4x].
///
/// \sa AeEnable
pub struct ExposureValue(pub f32);

impl TryFrom<f32> for ExposureValue {
    type Error = Infallible;

    fn try_from(value: f32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<f32> for ExposureValue {
    fn into(self) -> f32 {
        self.0
    }
}

impl Control for ExposureValue {
    type T = f32;
}

/// Exposure time (shutter speed) for the frame applied in the sensor
/// device. This value is specified in micro-seconds.
///
/// Setting this value means that it is now fixed and the AE algorithm may
/// not change it. Setting it back to zero returns it to the control of the
/// AE algorithm.
///
/// \sa AnalogueGain AeEnable
///
/// \todo Document the interactions between AeEnable and setting a fixed
/// value for this control. Consider interactions with other AE features,
/// such as aperture and aperture/shutter priority mode, and decide if
/// control of which features should be automatically adjusted shouldn't
/// better be handled through a separate AE mode control.
pub struct ExposureTime(pub i32);

impl TryFrom<i32> for ExposureTime {
    type Error = Infallible;

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<i32> for ExposureTime {
    fn into(self) -> i32 {
        self.0
    }
}

impl Control for ExposureTime {
    type T = i32;
}

/// Analogue gain value applied in the sensor device.
/// The value of the control specifies the gain multiplier applied to all
/// colour channels. This value cannot be lower than 1.0.
///
/// Setting this value means that it is now fixed and the AE algorithm may
/// not change it. Setting it back to zero returns it to the control of the
/// AE algorithm.
///
/// \sa ExposureTime AeEnable
///
/// \todo Document the interactions between AeEnable and setting a fixed
/// value for this control. Consider interactions with other AE features,
/// such as aperture and aperture/shutter priority mode, and decide if
/// control of which features should be automatically adjusted shouldn't
/// better be handled through a separate AE mode control.
pub struct AnalogueGain(pub f32);

impl TryFrom<f32> for AnalogueGain {
    type Error = Infallible;

    fn try_from(value: f32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<f32> for AnalogueGain {
    fn into(self) -> f32 {
        self.0
    }
}

impl Control for AnalogueGain {
    type T = f32;
}

/// Specify a fixed brightness parameter. Positive values (up to 1.0)
/// produce brighter images; negative values (up to -1.0) produce darker
/// images and 0.0 leaves pixels unchanged.
pub struct Brightness(pub f32);

impl TryFrom<f32> for Brightness {
    type Error = Infallible;

    fn try_from(value: f32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<f32> for Brightness {
    fn into(self) -> f32 {
        self.0
    }
}

impl Control for Brightness {
    type T = f32;
}

/// Specify a fixed contrast parameter. Normal contrast is given by the
/// value 1.0; larger values produce images with more contrast.
pub struct Contrast(pub f32);

impl TryFrom<f32> for Contrast {
    type Error = Infallible;

    fn try_from(value: f32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<f32> for Contrast {
    fn into(self) -> f32 {
        self.0
    }
}

impl Control for Contrast {
    type T = f32;
}

/// Report an estimate of the current illuminance level in lux. The Lux
/// control can only be returned in metadata.
pub struct Lux(pub f32);

impl TryFrom<f32> for Lux {
    type Error = Infallible;

    fn try_from(value: f32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<f32> for Lux {
    fn into(self) -> f32 {
        self.0
    }
}

impl Control for Lux {
    type T = f32;
}

/// Enable or disable the AWB.
///
/// \sa ColourGains
pub struct AwbEnable(pub bool);

impl TryFrom<bool> for AwbEnable {
    type Error = Infallible;

    fn try_from(value: bool) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<bool> for AwbEnable {
    fn into(self) -> bool {
        self.0
    }
}

impl Control for AwbEnable {
    type T = bool;
}

/// Specify the range of illuminants to use for the AWB algorithm. The modes
/// supported are platform specific, and not all modes may be supported.
pub enum AwbMode {
    /// Search over the whole colour temperature range.
    AwbAuto = 0,
    /// Incandescent AWB lamp mode.
    AwbIncandescent = 1,
    /// Tungsten AWB lamp mode.
    AwbTungsten = 2,
    /// Fluorescent AWB lamp mode.
    AwbFluorescent = 3,
    /// Indoor AWB lighting mode.
    AwbIndoor = 4,
    /// Daylight AWB lighting mode.
    AwbDaylight = 5,
    /// Cloudy AWB lighting mode.
    AwbCloudy = 6,
    /// Custom AWB mode.
    AwbCustom = 7,
}

impl TryFrom<i32> for AwbMode {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::AwbAuto),
            1 => Ok(Self::AwbIncandescent),
            2 => Ok(Self::AwbTungsten),
            3 => Ok(Self::AwbFluorescent),
            4 => Ok(Self::AwbIndoor),
            5 => Ok(Self::AwbDaylight),
            6 => Ok(Self::AwbCloudy),
            7 => Ok(Self::AwbCustom),
            _ => Err(()),
        }
    }
}

impl Into<i32> for AwbMode {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for AwbMode {
    type T = i32;
}

/// Report the lock status of a running AWB algorithm.
///
/// If the AWB algorithm is locked the value shall be set to true, if it's
/// converging it shall be set to false. If the AWB algorithm is not
/// running the control shall not be present in the metadata control list.
///
/// \sa AwbEnable
pub struct AwbLocked(pub bool);

impl TryFrom<bool> for AwbLocked {
    type Error = Infallible;

    fn try_from(value: bool) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<bool> for AwbLocked {
    fn into(self) -> bool {
        self.0
    }
}

impl Control for AwbLocked {
    type T = bool;
}

/// Pair of gain values for the Red and Blue colour channels, in that
/// order. ColourGains can only be applied in a Request when the AWB is
/// disabled.
///
/// \sa AwbEnable
pub struct ColourGains(pub f32);

impl TryFrom<f32> for ColourGains {
    type Error = Infallible;

    fn try_from(value: f32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<f32> for ColourGains {
    fn into(self) -> f32 {
        self.0
    }
}

impl Control for ColourGains {
    type T = f32;
}

/// Report the current estimate of the colour temperature, in kelvin, for this frame. The ColourTemperature control can only be returned in metadata.
pub struct ColourTemperature(pub i32);

impl TryFrom<i32> for ColourTemperature {
    type Error = Infallible;

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<i32> for ColourTemperature {
    fn into(self) -> i32 {
        self.0
    }
}

impl Control for ColourTemperature {
    type T = i32;
}

/// Specify a fixed saturation parameter. Normal saturation is given by
/// the value 1.0; larger values produce more saturated colours; 0.0
/// produces a greyscale image.
pub struct Saturation(pub f32);

impl TryFrom<f32> for Saturation {
    type Error = Infallible;

    fn try_from(value: f32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<f32> for Saturation {
    fn into(self) -> f32 {
        self.0
    }
}

impl Control for Saturation {
    type T = f32;
}

/// Reports the sensor black levels used for processing a frame, in the
/// order R, Gr, Gb, B. These values are returned as numbers out of a 16-bit
/// pixel range (as if pixels ranged from 0 to 65535). The SensorBlackLevels
/// control can only be returned in metadata.
pub struct SensorBlackLevels(pub i32);

impl TryFrom<i32> for SensorBlackLevels {
    type Error = Infallible;

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<i32> for SensorBlackLevels {
    fn into(self) -> i32 {
        self.0
    }
}

impl Control for SensorBlackLevels {
    type T = i32;
}

/// A value of 0.0 means no sharpening. The minimum value means
/// minimal sharpening, and shall be 0.0 unless the camera can't
/// disable sharpening completely. The default value shall give a
/// "reasonable" level of sharpening, suitable for most use cases.
/// The maximum value may apply extremely high levels of sharpening,
/// higher than anyone could reasonably want. Negative values are
/// not allowed. Note also that sharpening is not applied to raw
/// streams.
pub struct Sharpness(pub f32);

impl TryFrom<f32> for Sharpness {
    type Error = Infallible;

    fn try_from(value: f32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<f32> for Sharpness {
    fn into(self) -> f32 {
        self.0
    }
}

impl Control for Sharpness {
    type T = f32;
}

/// Reports a Figure of Merit (FoM) to indicate how in-focus the frame is.
/// A larger FocusFoM value indicates a more in-focus frame. This control
/// depends on the IPA to gather ISP statistics from the defined focus
/// region, and combine them in a suitable way to generate a FocusFoM value.
/// In this respect, it is not necessarily aimed at providing a way to
/// implement a focus algorithm by the application, rather an indication of
/// how in-focus a frame is.
pub struct FocusFoM(pub i32);

impl TryFrom<i32> for FocusFoM {
    type Error = Infallible;

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<i32> for FocusFoM {
    fn into(self) -> i32 {
        self.0
    }
}

impl Control for FocusFoM {
    type T = i32;
}

/// The 3x3 matrix that converts camera RGB to sRGB within the
/// imaging pipeline. This should describe the matrix that is used
/// after pixels have been white-balanced, but before any gamma
/// transformation. The 3x3 matrix is stored in conventional reading
/// order in an array of 9 floating point values.
pub struct ColourCorrectionMatrix(pub f32);

impl TryFrom<f32> for ColourCorrectionMatrix {
    type Error = Infallible;

    fn try_from(value: f32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<f32> for ColourCorrectionMatrix {
    fn into(self) -> f32 {
        self.0
    }
}

impl Control for ColourCorrectionMatrix {
    type T = f32;
}

/// Sets the image portion that will be scaled to form the whole of
/// the final output image. The (x,y) location of this rectangle is
/// relative to the PixelArrayActiveAreas that is being used. The units
/// remain native sensor pixels, even if the sensor is being used in
/// a binning or skipping mode.
///
/// This control is only present when the pipeline supports scaling. Its
/// maximum valid value is given by the properties::ScalerCropMaximum
/// property, and the two can be used to implement digital zoom.
pub struct ScalerCrop(pub ());

impl TryFrom<()> for ScalerCrop {
    type Error = Infallible;

    fn try_from(value: ()) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<()> for ScalerCrop {
    fn into(self) -> () {
        self.0
    }
}

impl Control for ScalerCrop {
    type T = ();
}

/// Digital gain value applied during the processing steps applied
/// to the image as captured from the sensor.
///
/// The global digital gain factor is applied to all the colour channels
/// of the RAW image. Different pipeline models are free to
/// specify how the global gain factor applies to each separate
/// channel.
///
/// If an imaging pipeline applies digital gain in distinct
/// processing steps, this value indicates their total sum.
/// Pipelines are free to decide how to adjust each processing
/// step to respect the received gain factor and shall report
/// their total value in the request metadata.
pub struct DigitalGain(pub f32);

impl TryFrom<f32> for DigitalGain {
    type Error = Infallible;

    fn try_from(value: f32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<f32> for DigitalGain {
    fn into(self) -> f32 {
        self.0
    }
}

impl Control for DigitalGain {
    type T = f32;
}

/// The instantaneous frame duration from start of frame exposure to start
/// of next exposure, expressed in microseconds. This control is meant to
/// be returned in metadata.
pub struct FrameDuration(pub i64);

impl TryFrom<i64> for FrameDuration {
    type Error = Infallible;

    fn try_from(value: i64) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<i64> for FrameDuration {
    fn into(self) -> i64 {
        self.0
    }
}

impl Control for FrameDuration {
    type T = i64;
}

/// The minimum and maximum (in that order) frame duration,
/// expressed in microseconds.
///
/// When provided by applications, the control specifies the sensor frame
/// duration interval the pipeline has to use. This limits the largest
/// exposure time the sensor can use. For example, if a maximum frame
/// duration of 33ms is requested (corresponding to 30 frames per second),
/// the sensor will not be able to raise the exposure time above 33ms.
/// A fixed frame duration is achieved by setting the minimum and maximum
/// values to be the same. Setting both values to 0 reverts to using the
/// IPA provided defaults.
///
/// The maximum frame duration provides the absolute limit to the shutter
/// speed computed by the AE algorithm and it overrides any exposure mode
/// setting specified with controls::AeExposureMode. Similarly, when a
/// manual exposure time is set through controls::ExposureTime, it also
/// gets clipped to the limits set by this control. When reported in
/// metadata, the control expresses the minimum and maximum frame
/// durations used after being clipped to the sensor provided frame
/// duration limits.
///
/// \sa AeExposureMode
/// \sa ExposureTime
///
/// \todo Define how to calculate the capture frame rate by
/// defining controls to report additional delays introduced by
/// the capture pipeline or post-processing stages (ie JPEG
/// conversion, frame scaling).
///
/// \todo Provide an explicit definition of default control values, for
/// this and all other controls.
pub struct FrameDurationLimits(pub i64);

impl TryFrom<i64> for FrameDurationLimits {
    type Error = Infallible;

    fn try_from(value: i64) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<i64> for FrameDurationLimits {
    fn into(self) -> i64 {
        self.0
    }
}

impl Control for FrameDurationLimits {
    type T = i64;
}

/// Temperature measure from the camera sensor in Celsius. This is typically
/// obtained by a thermal sensor present on-die or in the camera module. The
/// range of reported temperatures is device dependent.
///
/// The SensorTemperature control will only be returned in metadata if a
/// themal sensor is present.
pub struct SensorTemperature(pub f32);

impl TryFrom<f32> for SensorTemperature {
    type Error = Infallible;

    fn try_from(value: f32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<f32> for SensorTemperature {
    fn into(self) -> f32 {
        self.0
    }
}

impl Control for SensorTemperature {
    type T = f32;
}

/// The time when the first row of the image sensor active array is exposed.
///
/// The timestamp, expressed in nanoseconds, represents a monotonically
/// increasing counter since the system boot time, as defined by the
/// Linux-specific CLOCK_BOOTTIME clock id.
///
/// The SensorTimestamp control can only be returned in metadata.
///
/// \todo Define how the sensor timestamp has to be used in the reprocessing
/// use case.
pub struct SensorTimestamp(pub i64);

impl TryFrom<i64> for SensorTimestamp {
    type Error = Infallible;

    fn try_from(value: i64) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<i64> for SensorTimestamp {
    fn into(self) -> i64 {
        self.0
    }
}

impl Control for SensorTimestamp {
    type T = i64;
}

/// Control to set the mode of the AF (autofocus) algorithm.
///
/// An implementation may choose not to implement all the modes.
pub enum AfMode {
    /// The AF algorithm is in manual mode. In this mode it will never
    /// perform any action nor move the lens of its own accord, but an
    /// application can specify the desired lens position using the
    /// LensPosition control.
    ///
    /// In this mode the AfState will always report AfStateIdle.
    Manual = 0,
    /// The AF algorithm is in auto mode. This means that the algorithm
    /// will never move the lens or change state unless the AfTrigger
    /// control is used. The AfTrigger control can be used to initiate a
    /// focus scan, the results of which will be reported by AfState.
    ///
    /// If the autofocus algorithm is moved from AfModeAuto to another
    /// mode while a scan is in progress, the scan is cancelled
    /// immediately, without waiting for the scan to finish.
    ///
    /// When first entering this mode the AfState will report
    /// AfStateIdle. When a trigger control is sent, AfState will
    /// report AfStateScanning for a period before spontaneously
    /// changing to AfStateFocused or AfStateFailed, depending on
    /// the outcome of the scan. It will remain in this state until
    /// another scan is initiated by the AfTrigger control. If a scan is
    /// cancelled (without changing to another mode), AfState will return
    /// to AfStateIdle.
    Auto = 1,
    /// The AF algorithm is in continuous mode. This means that the lens can
    /// re-start a scan spontaneously at any moment, without any user
    /// intervention. The AfState still reports whether the algorithm is
    /// currently scanning or not, though the application has no ability to
    /// initiate or cancel scans, nor to move the lens for itself.
    ///
    /// However, applications can pause the AF algorithm from continuously
    /// scanning by using the AfPause control. This allows video or still
    /// images to be captured whilst guaranteeing that the focus is fixed.
    ///
    /// When set to AfModeContinuous, the system will immediately initiate a
    /// scan so AfState will report AfStateScanning, and will settle on one
    /// of AfStateFocused or AfStateFailed, depending on the scan result.
    Continuous = 2,
}

impl TryFrom<i32> for AfMode {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::Manual),
            1 => Ok(Self::Auto),
            2 => Ok(Self::Continuous),
            _ => Err(()),
        }
    }
}

impl Into<i32> for AfMode {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for AfMode {
    type T = i32;
}

/// Control to set the range of focus distances that is scanned. An
/// implementation may choose not to implement all the options here.
pub enum AfRange {
    /// A wide range of focus distances is scanned, all the way from
    /// infinity down to close distances, though depending on the
    /// implementation, possibly not including the very closest macro
    /// positions.
    Normal = 0,
    /// Only close distances are scanned.
    Macro = 1,
    /// The full range of focus distances is scanned just as with
    /// AfRangeNormal but this time including the very closest macro
    /// positions.
    Full = 2,
}

impl TryFrom<i32> for AfRange {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::Normal),
            1 => Ok(Self::Macro),
            2 => Ok(Self::Full),
            _ => Err(()),
        }
    }
}

impl Into<i32> for AfRange {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for AfRange {
    type T = i32;
}

/// Control that determines whether the AF algorithm is to move the lens
/// as quickly as possible or more steadily. For example, during video
/// recording it may be desirable not to move the lens too abruptly, but
/// when in a preview mode (waiting for a still capture) it may be
/// helpful to move the lens as quickly as is reasonably possible.
pub enum AfSpeed {
    /// Move the lens at its usual speed.
    Normal = 0,
    /// Move the lens more quickly.
    Fast = 1,
}

impl TryFrom<i32> for AfSpeed {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::Normal),
            1 => Ok(Self::Fast),
            _ => Err(()),
        }
    }
}

impl Into<i32> for AfSpeed {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for AfSpeed {
    type T = i32;
}

/// Instruct the AF algorithm how it should decide which parts of the image
/// should be used to measure focus.
pub enum AfMetering {
    /// The AF algorithm should decide for itself where it will measure focus.
    Auto = 0,
    /// The AF algorithm should use the rectangles defined by the AfWindows control to measure focus. If no windows are specified the behaviour is platform dependent.
    Windows = 1,
}

impl TryFrom<i32> for AfMetering {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::Auto),
            1 => Ok(Self::Windows),
            _ => Err(()),
        }
    }
}

impl Into<i32> for AfMetering {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for AfMetering {
    type T = i32;
}

/// Sets the focus windows used by the AF algorithm when AfMetering is set
/// to AfMeteringWindows. The units used are pixels within the rectangle
/// returned by the ScalerCropMaximum property.
///
/// In order to be activated, a rectangle must be programmed with non-zero
/// width and height. Internally, these rectangles are intersected with the
/// ScalerCropMaximum rectangle. If the window becomes empty after this
/// operation, then the window is ignored. If all the windows end up being
/// ignored, then the behaviour is platform dependent.
///
/// On platforms that support the ScalerCrop control (for implementing
/// digital zoom, for example), no automatic recalculation or adjustment of
/// AF windows is performed internally if the ScalerCrop is changed. If any
/// window lies outside the output image after the scaler crop has been
/// applied, it is up to the application to recalculate them.
///
/// The details of how the windows are used are platform dependent. We note
/// that when there is more than one AF window, a typical implementation
/// might find the optimal focus position for each one and finally select
/// the window where the focal distance for the objects shown in that part
/// of the image are closest to the camera.
pub struct AfWindows(pub ());

impl TryFrom<()> for AfWindows {
    type Error = Infallible;

    fn try_from(value: ()) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<()> for AfWindows {
    fn into(self) -> () {
        self.0
    }
}

impl Control for AfWindows {
    type T = ();
}

/// This control starts an autofocus scan when AfMode is set to AfModeAuto,
/// and can also be used to terminate a scan early.
///
/// It is ignored if AfMode is set to AfModeManual or AfModeContinuous.
pub enum AfTrigger {
    /// Start an AF scan. Ignored if a scan is in progress.
    Start = 0,
    /// Cancel an AF scan. This does not cause the lens to move anywhere else. Ignored if no scan is in progress.
    Cancel = 1,
}

impl TryFrom<i32> for AfTrigger {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::Start),
            1 => Ok(Self::Cancel),
            _ => Err(()),
        }
    }
}

impl Into<i32> for AfTrigger {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for AfTrigger {
    type T = i32;
}

/// This control has no effect except when in continuous autofocus mode
/// (AfModeContinuous). It can be used to pause any lens movements while
/// (for example) images are captured. The algorithm remains inactive
/// until it is instructed to resume.
pub enum AfPause {
    /// Pause the continuous autofocus algorithm immediately, whether or not
    /// any kind of scan is underway. AfPauseState will subsequently report
    /// AfPauseStatePaused. AfState may report any of AfStateScanning,
    /// AfStateFocused or AfStateFailed, depending on the algorithm's state
    /// when it received this control.
    Immediate = 0,
    /// This is similar to AfPauseImmediate, and if the AfState is currently
    /// reporting AfStateFocused or AfStateFailed it will remain in that
    /// state and AfPauseState will report AfPauseStatePaused.
    ///
    /// However, if the algorithm is scanning (AfStateScanning),
    /// AfPauseState will report AfPauseStatePausing until the scan is
    /// finished, at which point AfState will report one of AfStateFocused
    /// or AfStateFailed, and AfPauseState will change to
    /// AfPauseStatePaused.
    Deferred = 1,
    /// Resume continuous autofocus operation. The algorithm starts again
    /// from exactly where it left off, and AfPauseState will report
    /// AfPauseStateRunning.
    Resume = 2,
}

impl TryFrom<i32> for AfPause {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::Immediate),
            1 => Ok(Self::Deferred),
            2 => Ok(Self::Resume),
            _ => Err(()),
        }
    }
}

impl Into<i32> for AfPause {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for AfPause {
    type T = i32;
}

/// Acts as a control to instruct the lens to move to a particular position
/// and also reports back the position of the lens for each frame.
///
/// The LensPosition control is ignored unless the AfMode is set to
/// AfModeManual, though the value is reported back unconditionally in all
/// modes.
///
/// The units are a reciprocal distance scale like dioptres but normalised
/// for the hyperfocal distance. That is, for a lens with hyperfocal
/// distance H, and setting it to a focal distance D, the lens position LP,
/// which is generally a non-integer, is given by
///
/// \f$LP = \frac{H}{D}\f$
///
/// For example:
///
/// 0 moves the lens to infinity.
/// 0.5 moves the lens to twice the hyperfocal distance.
/// 1 moves the lens to the hyperfocal position.
/// And larger values will focus the lens ever closer.
///
/// \todo Define a property to report the Hyperforcal distance of calibrated
/// lenses.
///
/// \todo Define a property to report the maximum and minimum positions of
/// this lens. The minimum value will often be zero (meaning infinity).
pub struct LensPosition(pub f32);

impl TryFrom<f32> for LensPosition {
    type Error = Infallible;

    fn try_from(value: f32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<f32> for LensPosition {
    fn into(self) -> f32 {
        self.0
    }
}

impl Control for LensPosition {
    type T = f32;
}

/// Reports the current state of the AF algorithm in conjunction with the
/// reported AfMode value and (in continuous AF mode) the AfPauseState
/// value. The possible state changes are described below, though we note
/// the following state transitions that occur when the AfMode is changed.
///
/// If the AfMode is set to AfModeManual, then the AfState will always
/// report AfStateIdle (even if the lens is subsequently moved). Changing to
/// the AfModeManual state does not initiate any lens movement.
///
/// If the AfMode is set to AfModeAuto then the AfState will report
/// AfStateIdle. However, if AfModeAuto and AfTriggerStart are sent together
/// then AfState will omit AfStateIdle and move straight to AfStateScanning
/// (and start a scan).
///
/// If the AfMode is set to AfModeContinuous then the AfState will initially
/// report AfStateScanning.
pub enum AfState {
    /// The AF algorithm is in manual mode (AfModeManual) or in auto mode
    /// (AfModeAuto) and a scan has not yet been triggered, or an
    /// in-progress scan was cancelled.
    Idle = 0,
    /// The AF algorithm is in auto mode (AfModeAuto), and a scan has been
    /// started using the AfTrigger control. The scan can be cancelled by
    /// sending AfTriggerCancel at which point the algorithm will either
    /// move back to AfStateIdle or, if the scan actually completes before
    /// the cancel request is processed, to one of AfStateFocused or
    /// AfStateFailed.
    ///
    /// Alternatively the AF algorithm could be in continuous mode
    /// (AfModeContinuous) at which point it may enter this state
    /// spontaneously whenever it determines that a rescan is needed.
    Scanning = 1,
    /// The AF algorithm is in auto (AfModeAuto) or continuous
    /// (AfModeContinuous) mode and a scan has completed with the result
    /// that the algorithm believes the image is now in focus.
    Focused = 2,
    /// The AF algorithm is in auto (AfModeAuto) or continuous
    /// (AfModeContinuous) mode and a scan has completed with the result
    /// that the algorithm did not find a good focus position.
    Failed = 3,
}

impl TryFrom<i32> for AfState {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::Idle),
            1 => Ok(Self::Scanning),
            2 => Ok(Self::Focused),
            3 => Ok(Self::Failed),
            _ => Err(()),
        }
    }
}

impl Into<i32> for AfState {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for AfState {
    type T = i32;
}

/// Only applicable in continuous (AfModeContinuous) mode, this reports
/// whether the algorithm is currently running, paused or pausing (that is,
/// will pause as soon as any in-progress scan completes).
///
/// Any change to AfMode will cause AfPauseStateRunning to be reported.
pub enum AfPauseState {
    /// Continuous AF is running and the algorithm may restart a scan
    /// spontaneously.
    Running = 0,
    /// Continuous AF has been sent an AfPauseDeferred control, and will
    /// pause as soon as any in-progress scan completes (and then report
    /// AfPauseStatePaused). No new scans will be start spontaneously until
    /// the AfPauseResume control is sent.
    Pausing = 1,
    /// Continuous AF is paused. No further state changes or lens movements
    /// will occur until the AfPauseResume control is sent.
    Paused = 2,
}

impl TryFrom<i32> for AfPauseState {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::Running),
            1 => Ok(Self::Pausing),
            2 => Ok(Self::Paused),
            _ => Err(()),
        }
    }
}

impl Into<i32> for AfPauseState {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for AfPauseState {
    type T = i32;
}

/// Control for AE metering trigger. Currently identical to
/// ANDROID_CONTROL_AE_PRECAPTURE_TRIGGER.
///
/// Whether the camera device will trigger a precapture metering sequence
/// when it processes this request.
pub enum AePrecaptureTrigger {
    /// The trigger is idle.
    Idle = 0,
    /// The pre-capture AE metering is started by the camera.
    Start = 1,
    /// The camera will cancel any active or completed metering sequence.
    /// The AE algorithm is reset to its initial state.
    Cancel = 2,
}

impl TryFrom<i32> for AePrecaptureTrigger {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::Idle),
            1 => Ok(Self::Start),
            2 => Ok(Self::Cancel),
            _ => Err(()),
        }
    }
}

impl Into<i32> for AePrecaptureTrigger {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for AePrecaptureTrigger {
    type T = i32;
}

/// Control to select the noise reduction algorithm mode. Currently
/// identical to ANDROID_NOISE_REDUCTION_MODE.
///
///  Mode of operation for the noise reduction algorithm.
pub enum NoiseReductionMode {
    /// No noise reduction is applied
    Off = 0,
    /// Noise reduction is applied without reducing the frame rate.
    Fast = 1,
    /// High quality noise reduction at the expense of frame rate.
    HighQuality = 2,
    /// Minimal noise reduction is applied without reducing the frame rate.
    Minimal = 3,
    /// Noise reduction is applied at different levels to different streams.
    ZSL = 4,
}

impl TryFrom<i32> for NoiseReductionMode {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::Off),
            1 => Ok(Self::Fast),
            2 => Ok(Self::HighQuality),
            3 => Ok(Self::Minimal),
            4 => Ok(Self::ZSL),
            _ => Err(()),
        }
    }
}

impl Into<i32> for NoiseReductionMode {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for NoiseReductionMode {
    type T = i32;
}

/// Control to select the color correction aberration mode. Currently
/// identical to ANDROID_COLOR_CORRECTION_ABERRATION_MODE.
///
///  Mode of operation for the chromatic aberration correction algorithm.
pub enum ColorCorrectionAberrationMode {
    /// No aberration correction is applied.
    ColorCorrectionAberrationOff = 0,
    /// Aberration correction will not slow down the frame rate.
    ColorCorrectionAberrationFast = 1,
    /// High quality aberration correction which might reduce the frame
    /// rate.
    ColorCorrectionAberrationHighQuality = 2,
}

impl TryFrom<i32> for ColorCorrectionAberrationMode {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::ColorCorrectionAberrationOff),
            1 => Ok(Self::ColorCorrectionAberrationFast),
            2 => Ok(Self::ColorCorrectionAberrationHighQuality),
            _ => Err(()),
        }
    }
}

impl Into<i32> for ColorCorrectionAberrationMode {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for ColorCorrectionAberrationMode {
    type T = i32;
}

/// Control to report the current AE algorithm state. Currently identical to
/// ANDROID_CONTROL_AE_STATE.
///
///  Current state of the AE algorithm.
pub enum AeState {
    /// The AE algorithm is inactive.
    Inactive = 0,
    /// The AE algorithm has not converged yet.
    Searching = 1,
    /// The AE algorithm has converged.
    Converged = 2,
    /// The AE algorithm is locked.
    Locked = 3,
    /// The AE algorithm would need a flash for good results
    FlashRequired = 4,
    /// The AE algorithm has started a pre-capture metering session.
    /// \sa AePrecaptureTrigger
    Precapture = 5,
}

impl TryFrom<i32> for AeState {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::Inactive),
            1 => Ok(Self::Searching),
            2 => Ok(Self::Converged),
            3 => Ok(Self::Locked),
            4 => Ok(Self::FlashRequired),
            5 => Ok(Self::Precapture),
            _ => Err(()),
        }
    }
}

impl Into<i32> for AeState {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for AeState {
    type T = i32;
}

/// Control to report the current AWB algorithm state. Currently identical
/// to ANDROID_CONTROL_AWB_STATE.
///
///  Current state of the AWB algorithm.
pub enum AwbState {
    /// The AWB algorithm is inactive.
    Inactive = 0,
    /// The AWB algorithm has not converged yet.
    Searching = 1,
    /// The AWB algorithm has converged.
    AwbConverged = 2,
    /// The AWB algorithm is locked.
    AwbLocked = 3,
}

impl TryFrom<i32> for AwbState {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::Inactive),
            1 => Ok(Self::Searching),
            2 => Ok(Self::AwbConverged),
            3 => Ok(Self::AwbLocked),
            _ => Err(()),
        }
    }
}

impl Into<i32> for AwbState {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for AwbState {
    type T = i32;
}

/// Control to report the time between the start of exposure of the first
/// row and the start of exposure of the last row. Currently identical to
/// ANDROID_SENSOR_ROLLING_SHUTTER_SKEW
pub struct SensorRollingShutterSkew(pub i64);

impl TryFrom<i64> for SensorRollingShutterSkew {
    type Error = Infallible;

    fn try_from(value: i64) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<i64> for SensorRollingShutterSkew {
    fn into(self) -> i64 {
        self.0
    }
}

impl Control for SensorRollingShutterSkew {
    type T = i64;
}

/// Control to report if the lens shading map is available. Currently
/// identical to ANDROID_STATISTICS_LENS_SHADING_MAP_MODE.
pub enum LensShadingMapMode {
    /// No lens shading map mode is available.
    Off = 0,
    /// The lens shading map mode is available.
    On = 1,
}

impl TryFrom<i32> for LensShadingMapMode {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::Off),
            1 => Ok(Self::On),
            _ => Err(()),
        }
    }
}

impl Into<i32> for LensShadingMapMode {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for LensShadingMapMode {
    type T = i32;
}

/// Control to report the detected scene light frequency. Currently
/// identical to ANDROID_STATISTICS_SCENE_FLICKER.
pub enum SceneFlicker {
    /// No flickering detected.
    SceneFickerOff = 0,
    /// 50Hz flickering detected.
    SceneFicker50Hz = 1,
    /// 60Hz flickering detected.
    SceneFicker60Hz = 2,
}

impl TryFrom<i32> for SceneFlicker {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::SceneFickerOff),
            1 => Ok(Self::SceneFicker50Hz),
            2 => Ok(Self::SceneFicker60Hz),
            _ => Err(()),
        }
    }
}

impl Into<i32> for SceneFlicker {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for SceneFlicker {
    type T = i32;
}

/// Specifies the number of pipeline stages the frame went through from when
/// it was exposed to when the final completed result was available to the
/// framework. Always less than or equal to PipelineMaxDepth. Currently
/// identical to ANDROID_REQUEST_PIPELINE_DEPTH.
///
/// The typical value for this control is 3 as a frame is first exposed,
/// captured and then processed in a single pass through the ISP. Any
/// additional processing step performed after the ISP pass (in example face
/// detection, additional format conversions etc) count as an additional
/// pipeline stage.
pub struct PipelineDepth(pub i32);

impl TryFrom<i32> for PipelineDepth {
    type Error = Infallible;

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<i32> for PipelineDepth {
    fn into(self) -> i32 {
        self.0
    }
}

impl Control for PipelineDepth {
    type T = i32;
}

/// The maximum number of frames that can occur after a request (different
/// than the previous) has been submitted, and before the result's state
/// becomes synchronized. A value of -1 indicates unknown latency, and 0
/// indicates per-frame control. Currently identical to
/// ANDROID_SYNC_MAX_LATENCY.
pub struct MaxLatency(pub i32);

impl TryFrom<i32> for MaxLatency {
    type Error = Infallible;

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<i32> for MaxLatency {
    fn into(self) -> i32 {
        self.0
    }
}

impl Control for MaxLatency {
    type T = i32;
}

/// Control to select the test pattern mode. Currently identical to
/// ANDROID_SENSOR_TEST_PATTERN_MODE.
pub enum TestPatternMode {
    /// No test pattern mode is used. The camera device returns frames from
    /// the image sensor.
    Off = 0,
    /// Each pixel in [R, G_even, G_odd, B] is replaced by its respective
    /// color channel provided in test pattern data.
    /// \todo Add control for test pattern data.
    SolidColor = 1,
    /// All pixel data is replaced with an 8-bar color pattern. The vertical
    /// bars (left-to-right) are as follows; white, yellow, cyan, green,
    /// magenta, red, blue and black. Each bar should take up 1/8 of the
    /// sensor pixel array width. When this is not possible, the bar size
    /// should be rounded down to the nearest integer and the pattern can
    /// repeat on the right side. Each bar's height must always take up the
    /// full sensor pixel array height.
    ColorBars = 2,
    /// The test pattern is similar to TestPatternModeColorBars,
    /// except that each bar should start at its specified color at the top
    /// and fade to gray at the bottom. Furthermore each bar is further
    /// subdevided into a left and right half. The left half should have a
    /// smooth gradient, and the right half should have a quantized
    /// gradient. In particular, the right half's should consist of blocks
    /// of the same color for 1/16th active sensor pixel array width. The
    /// least significant bits in the quantized gradient should be copied
    /// from the most significant bits of the smooth gradient. The height of
    /// each bar should always be a multiple of 128. When this is not the
    /// case, the pattern should repeat at the bottom of the image.
    ColorBarsFadeToGray = 3,
    /// All pixel data is replaced by a pseudo-random sequence generated
    /// from a PN9 512-bit sequence (typically implemented in hardware with
    /// a linear feedback shift register). The generator should be reset at
    /// the beginning of each frame, and thus each subsequent raw frame with
    /// this test pattern should be exactly the same as the last.
    Pn9 = 4,
    /// The first custom test pattern. All custom patterns that are
    /// available only on this camera device are at least this numeric
    /// value. All of the custom test patterns will be static (that is the
    /// raw image must not vary from frame to frame).
    Custom1 = 256,
}

impl TryFrom<i32> for TestPatternMode {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::Off),
            1 => Ok(Self::SolidColor),
            2 => Ok(Self::ColorBars),
            3 => Ok(Self::ColorBarsFadeToGray),
            4 => Ok(Self::Pn9),
            256 => Ok(Self::Custom1),
            _ => Err(()),
        }
    }
}

impl Into<i32> for TestPatternMode {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for TestPatternMode {
    type T = i32;
}

pub enum PropertyId {
    /// Camera mounting location
    Location = 1,
    /// The camera rotation is expressed as the angular difference in degrees
    /// between two reference systems, one relative to the camera module, and
    /// one defined on the external world scene to be captured when projected
    /// on the image sensor pixel array.
    ///
    /// A camera sensor has a 2-dimensional reference system 'Rc' defined by
    /// its pixel array read-out order. The origin is set to the first pixel
    /// being read out, the X-axis points along the column read-out direction
    /// towards the last columns, and the Y-axis along the row read-out
    /// direction towards the last row.
    ///
    /// A typical example for a sensor with a 2592x1944 pixel array matrix
    /// observed from the front is
    ///
    ///             2591       X-axis          0
    ///               <------------------------+ 0
    ///               .......... ... ..........!
    ///               .......... ... ..........! Y-axis
    ///                          ...           !
    ///               .......... ... ..........!
    ///               .......... ... ..........! 1943
    ///                                        V
    ///
    ///
    /// The external world scene reference system 'Rs' is a 2-dimensional
    /// reference system on the focal plane of the camera module. The origin is
    /// placed on the top-left corner of the visible scene, the X-axis points
    /// towards the right, and the Y-axis points towards the bottom of the
    /// scene. The top, bottom, left and right directions are intentionally not
    /// defined and depend on the environment in which the camera is used.
    ///
    /// A typical example of a (very common) picture of a shark swimming from
    /// left to right, as seen from the camera, is
    ///
    ///              0               X-axis
    ///            0 +------------------------------------->
    ///              !
    ///              !
    ///              !
    ///              !           |\____)\___
    ///              !           ) _____  __`<
    ///              !           |/     )/
    ///              !
    ///              !
    ///              !
    ///              V
    ///            Y-axis
    ///
    /// With the reference system 'Rs' placed on the camera focal plane.
    ///
    ///                                 .!
    ///                             .    !
    ///                 _       .        !
    ///              +-/ \-+.            !
    ///              | (o) |                ! Camera focal plane
    ///              +-----+.            !
    ///                         .        !
    ///                             .    !
    ///                                 .!
    ///
    /// When projected on the sensor's pixel array, the image and the associated
    /// reference system 'Rs' are typically (but not always) inverted, due to
    /// the camera module's lens optical inversion effect.
    ///
    /// Assuming the above represented scene of the swimming shark, the lens
    /// inversion projects the scene and its reference system onto the sensor
    /// pixel array, seen from the front of the camera sensor, as follow
    ///
    ///           Y-axis
    ///              ^
    ///              !
    ///              !
    ///              !
    ///              !            |\_____)\__
    ///              !            ) ____  ___.<
    ///              !            |/    )/
    ///              !
    ///              !
    ///              !
    ///            0 +------------------------------------->
    ///              0               X-axis
    ///
    /// Note the shark being upside-down.
    ///
    /// The resulting projected reference system is named 'Rp'.
    ///
    /// The camera rotation property is then defined as the angular difference
    /// in the counter-clockwise direction between the camera reference system
    /// 'Rc' and the projected scene reference system 'Rp'. It is expressed in
    /// degrees as a number in the range [0, 360[.
    ///
    /// Examples
    ///
    /// 0 degrees camera rotation
    ///
    ///
    ///                   Y-Rp
    ///                    ^
    ///             Y-Rc   !
    ///              ^     !
    ///              !     !
    ///              !     !
    ///              !     !
    ///              !     !
    ///              !     !
    ///              !     !
    ///              !     !
    ///              !   0 +------------------------------------->
    ///              !     0               X-Rp
    ///            0 +------------------------------------->
    ///              0               X-Rc
    ///
    ///
    ///                               X-Rc                0
    ///              <------------------------------------+ 0
    ///                          X-Rp                 0   !
    ///          <------------------------------------+ 0 !
    ///                                               !   !
    ///                                               !   !
    ///                                               !   !
    ///                                               !   !
    ///                                               !   !
    ///                                               !   !
    ///                                               !   !
    ///                                               !   V
    ///                                               !  Y-Rc
    ///                                               V
    ///                                              Y-Rp
    ///
    /// 90 degrees camera rotation
    ///
    ///              0        Y-Rc
    ///            0 +-------------------->
    ///              !   Y-Rp
    ///              !    ^
    ///              !    !
    ///              !    !
    ///              !    !
    ///              !    !
    ///              !    !
    ///              !    !
    ///              !    !
    ///              !    !
    ///              !    !
    ///              !  0 +------------------------------------->
    ///              !    0              X-Rp
    ///              !
    ///              !
    ///              !
    ///              !
    ///              V
    ///             X-Rc
    ///
    /// 180 degrees camera rotation
    ///
    ///                                           0
    ///      <------------------------------------+ 0
    ///                       X-Rc                !
    ///             Y-Rp                          !
    ///              ^                            !
    ///              !                            !
    ///              !                            !
    ///              !                            !
    ///              !                            !
    ///              !                            !
    ///              !                            !
    ///              !                            V
    ///              !                           Y-Rc
    ///            0 +------------------------------------->
    ///              0              X-Rp
    ///
    /// 270 degrees camera rotation
    ///
    ///              0        Y-Rc
    ///            0 +-------------------->
    ///              !                                        0
    ///              !    <-----------------------------------+ 0
    ///              !                    X-Rp                !
    ///              !                                        !
    ///              !                                        !
    ///              !                                        !
    ///              !                                        !
    ///              !                                        !
    ///              !                                        !
    ///              !                                        !
    ///              !                                        !
    ///              !                                        V
    ///              !                                       Y-Rp
    ///              !
    ///              !
    ///              !
    ///              !
    ///              V
    ///             X-Rc
    ///
    ///
    /// Example one - Webcam
    ///
    /// A camera module installed on the user facing part of a laptop screen
    /// casing used for video calls. The captured images are meant to be
    /// displayed in landscape mode (width > height) on the laptop screen.
    ///
    /// The camera is typically mounted upside-down to compensate the lens
    /// optical inversion effect.
    ///
    ///                   Y-Rp
    ///             Y-Rc   ^
    ///              ^     !
    ///              !     !
    ///              !     !       |\_____)\__
    ///              !     !       ) ____  ___.<
    ///              !     !       |/    )/
    ///              !     !
    ///              !     !
    ///              !     !
    ///              !   0 +------------------------------------->
    ///              !     0           X-Rp
    ///            0 +------------------------------------->
    ///              0            X-Rc
    ///
    /// The two reference systems are aligned, the resulting camera rotation is
    /// 0 degrees, no rotation correction needs to be applied to the resulting
    /// image once captured to memory buffers to correctly display it to users.
    ///
    ///              +--------------------------------------+
    ///              !                                      !
    ///              !                                      !
    ///              !                                      !
    ///              !             |\____)\___              !
    ///              !             ) _____  __`<            !
    ///              !             |/     )/                !
    ///              !                                      !
    ///              !                                      !
    ///              !                                      !
    ///              +--------------------------------------+
    ///
    /// If the camera sensor is not mounted upside-down to compensate for the
    /// lens optical inversion, the two reference systems will not be aligned,
    /// with 'Rp' being rotated 180 degrees relatively to 'Rc'.
    ///
    ///
    ///                       X-Rc                0
    ///      <------------------------------------+ 0
    ///                                           !
    ///             Y-Rp                          !
    ///              ^                            !
    ///              !                            !
    ///              !       |\_____)\__          !
    ///              !       ) ____  ___.<        !
    ///              !       |/    )/             !
    ///              !                            !
    ///              !                            !
    ///              !                            V
    ///              !                           Y-Rc
    ///            0 +------------------------------------->
    ///              0            X-Rp
    ///
    /// The image once captured to memory will then be rotated by 180 degrees
    ///
    ///              +--------------------------------------+
    ///              !                                      !
    ///              !                                      !
    ///              !                                      !
    ///              !              __/(_____/|             !
    ///              !            >.___  ____ (             !
    ///              !                 \(    \|             !
    ///              !                                      !
    ///              !                                      !
    ///              !                                      !
    ///              +--------------------------------------+
    ///
    /// A software rotation correction of 180 degrees should be applied to
    /// correctly display the image.
    ///
    ///              +--------------------------------------+
    ///              !                                      !
    ///              !                                      !
    ///              !                                      !
    ///              !             |\____)\___              !
    ///              !             ) _____  __`<            !
    ///              !             |/     )/                !
    ///              !                                      !
    ///              !                                      !
    ///              !                                      !
    ///              +--------------------------------------+
    ///
    /// Example two - Phone camera
    ///
    /// A camera installed on the back side of a mobile device facing away from
    /// the user. The captured images are meant to be displayed in portrait mode
    /// (height > width) to match the device screen orientation and the device
    /// usage orientation used when taking the picture.
    ///
    /// The camera sensor is typically mounted with its pixel array longer side
    /// aligned to the device longer side, upside-down mounted to compensate for
    /// the lens optical inversion effect.
    ///
    ///              0        Y-Rc
    ///            0 +-------------------->
    ///              !   Y-Rp
    ///              !    ^
    ///              !    !
    ///              !    !
    ///              !    !
    ///              !    !            |\_____)\__
    ///              !    !            ) ____  ___.<
    ///              !    !            |/    )/
    ///              !    !
    ///              !    !
    ///              !    !
    ///              !  0 +------------------------------------->
    ///              !    0                X-Rp
    ///              !
    ///              !
    ///              !
    ///              !
    ///              V
    ///             X-Rc
    ///
    /// The two reference systems are not aligned and the 'Rp' reference
    /// system is rotated by 90 degrees in the counter-clockwise direction
    /// relatively to the 'Rc' reference system.
    ///
    /// The image once captured to memory will be rotated.
    ///
    ///              +-------------------------------------+
    ///              |                 _ _                 |
    ///              |                \   /                |
    ///              |                 | |                 |
    ///              |                 | |                 |
    ///              |                 |  >                |
    ///              |                <  |                 |
    ///              |                 | |                 |
    ///              |                   .                 |
    ///              |                  V                  |
    ///              +-------------------------------------+
    ///
    /// A correction of 90 degrees in counter-clockwise direction has to be
    /// applied to correctly display the image in portrait mode on the device
    /// screen.
    ///
    ///                       +--------------------+
    ///                       |                    |
    ///                       |                    |
    ///                       |                    |
    ///                       |                    |
    ///                       |                    |
    ///                       |                    |
    ///                       |   |\____)\___      |
    ///                       |   ) _____  __`<    |
    ///                       |   |/     )/        |
    ///                       |                    |
    ///                       |                    |
    ///                       |                    |
    ///                       |                    |
    ///                       |                    |
    ///                       +--------------------+
    Rotation = 2,
    /// The model name shall to the extent possible describe the sensor. For
    /// most devices this is the model name of the sensor. While for some
    /// devices the sensor model is unavailable as the sensor or the entire
    /// camera is part of a larger unit and exposed as a black-box to the
    /// system. In such cases the model name of the smallest device that
    /// contains the camera sensor shall be used.
    ///
    /// The model name is not meant to be a camera name displayed to the
    /// end-user, but may be combined with other camera information to create a
    /// camera name.
    ///
    /// The model name is not guaranteed to be unique in the system nor is
    /// it guaranteed to be stable or have any other properties required to make
    /// it a good candidate to be used as a permanent identifier of a camera.
    ///
    /// The model name shall describe the camera in a human readable format and
    /// shall be encoded in ASCII.
    ///
    /// Example model names are 'ov5670', 'imx219' or 'Logitech Webcam C930e'.
    Model = 3,
    /// The pixel unit cell physical size, in nanometers.
    ///
    /// The UnitCellSize properties defines the horizontal and vertical sizes of
    /// a single pixel unit, including its active and non-active parts. In
    /// other words, it expresses the horizontal and vertical distance between
    /// the top-left corners of adjacent pixels.
    ///
    /// The property can be used to calculate the physical size of the sensor's
    /// pixel array area and for calibration purposes.
    UnitCellSize = 4,
    /// The camera sensor pixel array readable area vertical and horizontal
    /// sizes, in pixels.
    ///
    /// The PixelArraySize property defines the size in pixel units of the
    /// readable part of full pixel array matrix, including optical black
    /// pixels used for calibration, pixels which are not considered valid for
    /// capture and active pixels containing valid image data.
    ///
    /// The property describes the maximum size of the raw data captured by the
    /// camera, which might not correspond to the physical size of the sensor
    /// pixel array matrix, as some portions of the physical pixel array matrix
    /// are not accessible and cannot be transmitted out.
    ///
    /// For example, let's consider a pixel array matrix assembled as follows
    ///
    ///      +--------------------------------------------------+
    ///      |xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx|
    ///      |xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx|
    ///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
    ///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
    ///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
    ///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
    ///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
    ///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
    ///      ...          ...           ...      ...          ...
    ///
    ///      ...          ...           ...      ...          ...
    ///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
    ///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
    ///      |xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx|
    ///      |xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx|
    ///      +--------------------------------------------------+
    ///
    /// starting with two lines of non-readable pixels (x), followed by N lines
    /// of readable data (D) surrounded by two columns of non-readable pixels on
    /// each side, and ending with two more lines of non-readable pixels. Only
    /// the readable portion is transmitted to the receiving side, defining the
    /// sizes of the largest possible buffer of raw data that can be presented
    /// to applications.
    ///
    ///                      PixelArraySize.width
    ///        /----------------------------------------------/
    ///        +----------------------------------------------+ /
    ///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| |
    ///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| |
    ///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| |
    ///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| |
    ///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| |
    ///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| | PixelArraySize.height
    ///        ...        ...           ...      ...        ...
    ///        ...        ...           ...      ...        ...
    ///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| |
    ///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| |
    ///        +----------------------------------------------+ /
    ///
    /// This defines a rectangle whose top-left corner is placed in position (0,
    /// 0) and whose vertical and horizontal sizes are defined by this property.
    /// All other rectangles that describe portions of the pixel array, such as
    /// the optical black pixels rectangles and active pixel areas, are defined
    /// relatively to this rectangle.
    ///
    /// All the coordinates are expressed relative to the default sensor readout
    /// direction, without any transformation (such as horizontal and vertical
    /// flipping) applied. When mapping them to the raw pixel buffer,
    /// applications shall take any configured transformation into account.
    ///
    /// \todo Rename this property to Size once we will have property
    ///       categories (i.e. Properties::PixelArray::Size)
    PixelArraySize = 5,
    /// The pixel array region(s) which contain optical black pixels
    /// considered valid for calibration purposes.
    ///
    /// This property describes the position and size of optical black pixel
    /// regions in the raw data buffer as stored in memory, which might differ
    /// from their actual physical location in the pixel array matrix.
    ///
    /// It is important to note, in fact, that camera sensors might
    /// automatically reorder or skip portions of their pixels array matrix when
    /// transmitting data to the receiver. For instance, a sensor may merge the
    /// top and bottom optical black rectangles into a single rectangle,
    /// transmitted at the beginning of the frame.
    ///
    /// The pixel array contains several areas with different purposes,
    /// interleaved by lines and columns which are said not to be valid for
    /// capturing purposes. Invalid lines and columns are defined as invalid as
    /// they could be positioned too close to the chip margins or to the optical
    /// black shielding placed on top of optical black pixels.
    ///
    ///                      PixelArraySize.width
    ///        /----------------------------------------------/
    ///           x1                                       x2
    ///        +--o---------------------------------------o---+ /
    ///        |IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII| |
    ///        |IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII| |
    ///     y1 oIIOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOII| |
    ///        |IIOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOII| |
    ///        |IIOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOII| |
    ///     y2 oIIOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOII| |
    ///        |IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII| |
    ///        |IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII| |
    ///     y3 |IIOOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOOII| |
    ///        |IIOOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOOII| | PixelArraySize.height
    ///        |IIOOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOOII| |
    ///        ...          ...           ...     ...       ...
    ///        ...          ...           ...     ...       ...
    ///     y4 |IIOOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOOII| |
    ///        |IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII| |
    ///        |IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII| |
    ///        +----------------------------------------------+ /
    ///
    /// The readable pixel array matrix is composed by
    /// 2 invalid lines (I)
    /// 4 lines of valid optical black pixels (O)
    /// 2 invalid lines (I)
    /// n lines of valid pixel data (P)
    /// 2 invalid lines (I)
    ///
    /// And the position of the optical black pixel rectangles is defined by
    ///
    ///     PixelArrayOpticalBlackRectangles = {
    ///        { x1, y1, x2 - x1 + 1, y2 - y1 + 1 },
    ///        { x1, y3, 2, y4 - y3 + 1 },
    ///        { x2, y3, 2, y4 - y3 + 1 },
    ///     };
    ///
    /// If the camera, when capturing the full pixel array matrix, automatically
    /// skips the invalid lines and columns, producing the following data
    /// buffer, when captured to memory
    ///
    ///                      PixelArraySize.width
    ///        /----------------------------------------------/
    ///                                                    x1
    ///        +--------------------------------------------o-+ /
    ///        |OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO| |
    ///        |OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO| |
    ///        |OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO| |
    ///        |OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO| |
    ///     y1 oOOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOO| |
    ///        |OOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOO| |
    ///        |OOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOO| | PixelArraySize.height
    ///        ...       ...          ...       ...         ... |
    ///        ...       ...          ...       ...         ... |
    ///        |OOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOO| |
    ///        |OOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOO| |
    ///        +----------------------------------------------+ /
    ///
    /// then the invalid lines and columns should not be reported as part of the
    /// PixelArraySize property in first place.
    ///
    /// In this case, the position of the black pixel rectangles will be
    ///
    ///     PixelArrayOpticalBlackRectangles = {
    ///        { 0, 0, y1 + 1, PixelArraySize[0] },
    ///        { 0, y1, 2, PixelArraySize[1] - y1 + 1 },
    ///        { x1, y1, 2, PixelArraySize[1] - y1 + 1 },
    ///     };
    ///
    /// \todo Rename this property to Size once we will have property
    ///       categories (i.e. Properties::PixelArray::OpticalBlackRectangles)
    PixelArrayOpticalBlackRectangles = 6,
    /// The PixelArrayActiveAreas property defines the (possibly multiple and
    /// overlapping) portions of the camera sensor readable pixel matrix
    /// which are considered valid for image acquisition purposes.
    ///
    /// This property describes an arbitrary number of overlapping rectangles,
    /// with each rectangle representing the maximum image size that the camera
    /// sensor can produce for a particular aspect ratio. They are defined
    /// relatively to the PixelArraySize rectangle.
    ///
    /// When multiple rectangles are reported, they shall be ordered from the
    /// tallest to the shortest.
    ///
    /// Example 1
    /// A camera sensor which only produces images in the 4:3 image resolution
    /// will report a single PixelArrayActiveAreas rectangle, from which all
    /// other image formats are obtained by either cropping the field-of-view
    /// and/or applying pixel sub-sampling techniques such as pixel skipping or
    /// binning.
    ///
    ///            PixelArraySize.width
    ///             /----------------/
    ///               x1          x2
    ///     (0,0)-> +-o------------o-+  /
    ///          y1 o +------------+ |  |
    ///             | |////////////| |  |
    ///             | |////////////| |  | PixelArraySize.height
    ///             | |////////////| |  |
    ///          y2 o +------------+ |  |
    ///             +----------------+  /
    ///
    /// The property reports a single rectangle
    ///
    ///          PixelArrayActiveAreas = (x1, y1, x2 - x1 + 1, y2 - y1 + 1)
    ///
    /// Example 2
    /// A camera sensor which can produce images in different native
    /// resolutions will report several overlapping rectangles, one for each
    /// natively supported resolution.
    ///
    ///              PixelArraySize.width
    ///             /------------------/
    ///               x1  x2    x3  x4
    ///     (0,0)-> +o---o------o---o+  /
    ///          y1 o    +------+    |  |
    ///             |    |//////|    |  |
    ///          y2 o+---+------+---+|  |
    ///             ||///|//////|///||  | PixelArraySize.height
    ///          y3 o+---+------+---+|  |
    ///             |    |//////|    |  |
    ///          y4 o    +------+    |  |
    ///             +----+------+----+  /
    ///
    /// The property reports two rectangles
    ///
    ///         PixelArrayActiveAreas = ((x2, y1, x3 - x2 + 1, y4 - y1 + 1),
    ///                                  (x1, y2, x4 - x1 + 1, y3 - y2 + 1))
    ///
    /// The first rectangle describes the maximum field-of-view of all image
    /// formats in the 4:3 resolutions, while the second one describes the
    /// maximum field of view for all image formats in the 16:9 resolutions.
    ///
    /// Multiple rectangles shall only be reported when the sensor can't capture
    /// the pixels in the corner regions. If all the pixels in the (x1,y1) -
    /// (x4,y4) area can be captured, the PixelArrayActiveAreas property shall
    /// contains the single rectangle (x1,y1) - (x4,y4).
    ///
    /// \todo Rename this property to ActiveAreas once we will have property
    ///       categories (i.e. Properties::PixelArray::ActiveAreas)
    PixelArrayActiveAreas = 7,
    /// The maximum valid rectangle for the controls::ScalerCrop control. This
    /// reflects the minimum mandatory cropping applied in the camera sensor and
    /// the rest of the pipeline. Just as the ScalerCrop control, it defines a
    /// rectangle taken from the sensor's active pixel array.
    ///
    /// This property is valid only after the camera has been successfully
    /// configured and its value may change whenever a new configuration is
    /// applied.
    ///
    /// \todo Turn this property into a "maximum control value" for the
    /// ScalerCrop control once "dynamic" controls have been implemented.
    ScalerCropMaximum = 8,
    /// The relative sensitivity of the chosen sensor mode.
    ///
    /// Some sensors have readout modes with different sensitivities. For example,
    /// a binned camera mode might, with the same exposure and gains, produce
    /// twice the signal level of the full resolution readout. This would be
    /// signalled by the binned mode, when it is chosen, indicating a value here
    /// that is twice that of the full resolution mode. This value will be valid
    /// after the configure method has returned successfully.
    SensorSensitivity = 9,
    /// The arrangement of color filters on sensor; represents the colors in the
    /// top-left 2x2 section of the sensor, in reading order. Currently
    /// identical to ANDROID_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT.
    ColorFilterArrangement = 10,
}

/// Camera mounting location
pub enum Location {
    /// The camera is mounted on the front side of the device, facing the
    /// user
    CameraFront = 0,
    /// The camera is mounted on the back side of the device, facing away
    /// from the user
    CameraBack = 1,
    /// The camera is attached to the device in a way that allows it to
    /// be moved freely
    CameraExternal = 2,
}

impl TryFrom<i32> for Location {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::CameraFront),
            1 => Ok(Self::CameraBack),
            2 => Ok(Self::CameraExternal),
            _ => Err(()),
        }
    }
}

impl Into<i32> for Location {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for Location {
    type T = i32;
}

/// The camera rotation is expressed as the angular difference in degrees
/// between two reference systems, one relative to the camera module, and
/// one defined on the external world scene to be captured when projected
/// on the image sensor pixel array.
///
/// A camera sensor has a 2-dimensional reference system 'Rc' defined by
/// its pixel array read-out order. The origin is set to the first pixel
/// being read out, the X-axis points along the column read-out direction
/// towards the last columns, and the Y-axis along the row read-out
/// direction towards the last row.
///
/// A typical example for a sensor with a 2592x1944 pixel array matrix
/// observed from the front is
///
///             2591       X-axis          0
///               <------------------------+ 0
///               .......... ... ..........!
///               .......... ... ..........! Y-axis
///                          ...           !
///               .......... ... ..........!
///               .......... ... ..........! 1943
///                                        V
///
///
/// The external world scene reference system 'Rs' is a 2-dimensional
/// reference system on the focal plane of the camera module. The origin is
/// placed on the top-left corner of the visible scene, the X-axis points
/// towards the right, and the Y-axis points towards the bottom of the
/// scene. The top, bottom, left and right directions are intentionally not
/// defined and depend on the environment in which the camera is used.
///
/// A typical example of a (very common) picture of a shark swimming from
/// left to right, as seen from the camera, is
///
///              0               X-axis
///            0 +------------------------------------->
///              !
///              !
///              !
///              !           |\____)\___
///              !           ) _____  __`<
///              !           |/     )/
///              !
///              !
///              !
///              V
///            Y-axis
///
/// With the reference system 'Rs' placed on the camera focal plane.
///
///                                 .!
///                             .    !
///                 _       .        !
///              +-/ \-+.            !
///              | (o) |                ! Camera focal plane
///              +-----+.            !
///                         .        !
///                             .    !
///                                 .!
///
/// When projected on the sensor's pixel array, the image and the associated
/// reference system 'Rs' are typically (but not always) inverted, due to
/// the camera module's lens optical inversion effect.
///
/// Assuming the above represented scene of the swimming shark, the lens
/// inversion projects the scene and its reference system onto the sensor
/// pixel array, seen from the front of the camera sensor, as follow
///
///           Y-axis
///              ^
///              !
///              !
///              !
///              !            |\_____)\__
///              !            ) ____  ___.<
///              !            |/    )/
///              !
///              !
///              !
///            0 +------------------------------------->
///              0               X-axis
///
/// Note the shark being upside-down.
///
/// The resulting projected reference system is named 'Rp'.
///
/// The camera rotation property is then defined as the angular difference
/// in the counter-clockwise direction between the camera reference system
/// 'Rc' and the projected scene reference system 'Rp'. It is expressed in
/// degrees as a number in the range [0, 360[.
///
/// Examples
///
/// 0 degrees camera rotation
///
///
///                   Y-Rp
///                    ^
///             Y-Rc   !
///              ^     !
///              !     !
///              !     !
///              !     !
///              !     !
///              !     !
///              !     !
///              !     !
///              !   0 +------------------------------------->
///              !     0               X-Rp
///            0 +------------------------------------->
///              0               X-Rc
///
///
///                               X-Rc                0
///              <------------------------------------+ 0
///                          X-Rp                 0   !
///          <------------------------------------+ 0 !
///                                               !   !
///                                               !   !
///                                               !   !
///                                               !   !
///                                               !   !
///                                               !   !
///                                               !   !
///                                               !   V
///                                               !  Y-Rc
///                                               V
///                                              Y-Rp
///
/// 90 degrees camera rotation
///
///              0        Y-Rc
///            0 +-------------------->
///              !   Y-Rp
///              !    ^
///              !    !
///              !    !
///              !    !
///              !    !
///              !    !
///              !    !
///              !    !
///              !    !
///              !    !
///              !  0 +------------------------------------->
///              !    0              X-Rp
///              !
///              !
///              !
///              !
///              V
///             X-Rc
///
/// 180 degrees camera rotation
///
///                                           0
///      <------------------------------------+ 0
///                       X-Rc                !
///             Y-Rp                          !
///              ^                            !
///              !                            !
///              !                            !
///              !                            !
///              !                            !
///              !                            !
///              !                            !
///              !                            V
///              !                           Y-Rc
///            0 +------------------------------------->
///              0              X-Rp
///
/// 270 degrees camera rotation
///
///              0        Y-Rc
///            0 +-------------------->
///              !                                        0
///              !    <-----------------------------------+ 0
///              !                    X-Rp                !
///              !                                        !
///              !                                        !
///              !                                        !
///              !                                        !
///              !                                        !
///              !                                        !
///              !                                        !
///              !                                        !
///              !                                        V
///              !                                       Y-Rp
///              !
///              !
///              !
///              !
///              V
///             X-Rc
///
///
/// Example one - Webcam
///
/// A camera module installed on the user facing part of a laptop screen
/// casing used for video calls. The captured images are meant to be
/// displayed in landscape mode (width > height) on the laptop screen.
///
/// The camera is typically mounted upside-down to compensate the lens
/// optical inversion effect.
///
///                   Y-Rp
///             Y-Rc   ^
///              ^     !
///              !     !
///              !     !       |\_____)\__
///              !     !       ) ____  ___.<
///              !     !       |/    )/
///              !     !
///              !     !
///              !     !
///              !   0 +------------------------------------->
///              !     0           X-Rp
///            0 +------------------------------------->
///              0            X-Rc
///
/// The two reference systems are aligned, the resulting camera rotation is
/// 0 degrees, no rotation correction needs to be applied to the resulting
/// image once captured to memory buffers to correctly display it to users.
///
///              +--------------------------------------+
///              !                                      !
///              !                                      !
///              !                                      !
///              !             |\____)\___              !
///              !             ) _____  __`<            !
///              !             |/     )/                !
///              !                                      !
///              !                                      !
///              !                                      !
///              +--------------------------------------+
///
/// If the camera sensor is not mounted upside-down to compensate for the
/// lens optical inversion, the two reference systems will not be aligned,
/// with 'Rp' being rotated 180 degrees relatively to 'Rc'.
///
///
///                       X-Rc                0
///      <------------------------------------+ 0
///                                           !
///             Y-Rp                          !
///              ^                            !
///              !                            !
///              !       |\_____)\__          !
///              !       ) ____  ___.<        !
///              !       |/    )/             !
///              !                            !
///              !                            !
///              !                            V
///              !                           Y-Rc
///            0 +------------------------------------->
///              0            X-Rp
///
/// The image once captured to memory will then be rotated by 180 degrees
///
///              +--------------------------------------+
///              !                                      !
///              !                                      !
///              !                                      !
///              !              __/(_____/|             !
///              !            >.___  ____ (             !
///              !                 \(    \|             !
///              !                                      !
///              !                                      !
///              !                                      !
///              +--------------------------------------+
///
/// A software rotation correction of 180 degrees should be applied to
/// correctly display the image.
///
///              +--------------------------------------+
///              !                                      !
///              !                                      !
///              !                                      !
///              !             |\____)\___              !
///              !             ) _____  __`<            !
///              !             |/     )/                !
///              !                                      !
///              !                                      !
///              !                                      !
///              +--------------------------------------+
///
/// Example two - Phone camera
///
/// A camera installed on the back side of a mobile device facing away from
/// the user. The captured images are meant to be displayed in portrait mode
/// (height > width) to match the device screen orientation and the device
/// usage orientation used when taking the picture.
///
/// The camera sensor is typically mounted with its pixel array longer side
/// aligned to the device longer side, upside-down mounted to compensate for
/// the lens optical inversion effect.
///
///              0        Y-Rc
///            0 +-------------------->
///              !   Y-Rp
///              !    ^
///              !    !
///              !    !
///              !    !
///              !    !            |\_____)\__
///              !    !            ) ____  ___.<
///              !    !            |/    )/
///              !    !
///              !    !
///              !    !
///              !  0 +------------------------------------->
///              !    0                X-Rp
///              !
///              !
///              !
///              !
///              V
///             X-Rc
///
/// The two reference systems are not aligned and the 'Rp' reference
/// system is rotated by 90 degrees in the counter-clockwise direction
/// relatively to the 'Rc' reference system.
///
/// The image once captured to memory will be rotated.
///
///              +-------------------------------------+
///              |                 _ _                 |
///              |                \   /                |
///              |                 | |                 |
///              |                 | |                 |
///              |                 |  >                |
///              |                <  |                 |
///              |                 | |                 |
///              |                   .                 |
///              |                  V                  |
///              +-------------------------------------+
///
/// A correction of 90 degrees in counter-clockwise direction has to be
/// applied to correctly display the image in portrait mode on the device
/// screen.
///
///                       +--------------------+
///                       |                    |
///                       |                    |
///                       |                    |
///                       |                    |
///                       |                    |
///                       |                    |
///                       |   |\____)\___      |
///                       |   ) _____  __`<    |
///                       |   |/     )/        |
///                       |                    |
///                       |                    |
///                       |                    |
///                       |                    |
///                       |                    |
///                       +--------------------+
pub struct Rotation(pub i32);

impl TryFrom<i32> for Rotation {
    type Error = Infallible;

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<i32> for Rotation {
    fn into(self) -> i32 {
        self.0
    }
}

impl Control for Rotation {
    type T = i32;
}

/// The model name shall to the extent possible describe the sensor. For
/// most devices this is the model name of the sensor. While for some
/// devices the sensor model is unavailable as the sensor or the entire
/// camera is part of a larger unit and exposed as a black-box to the
/// system. In such cases the model name of the smallest device that
/// contains the camera sensor shall be used.
///
/// The model name is not meant to be a camera name displayed to the
/// end-user, but may be combined with other camera information to create a
/// camera name.
///
/// The model name is not guaranteed to be unique in the system nor is
/// it guaranteed to be stable or have any other properties required to make
/// it a good candidate to be used as a permanent identifier of a camera.
///
/// The model name shall describe the camera in a human readable format and
/// shall be encoded in ASCII.
///
/// Example model names are 'ov5670', 'imx219' or 'Logitech Webcam C930e'.
pub struct Model(pub String);

impl TryFrom<String> for Model {
    type Error = Infallible;

    fn try_from(value: String) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<String> for Model {
    fn into(self) -> String {
        self.0
    }
}

impl Control for Model {
    type T = String;
}

/// The pixel unit cell physical size, in nanometers.
///
/// The UnitCellSize properties defines the horizontal and vertical sizes of
/// a single pixel unit, including its active and non-active parts. In
/// other words, it expresses the horizontal and vertical distance between
/// the top-left corners of adjacent pixels.
///
/// The property can be used to calculate the physical size of the sensor's
/// pixel array area and for calibration purposes.
pub struct UnitCellSize(pub ());

impl TryFrom<()> for UnitCellSize {
    type Error = Infallible;

    fn try_from(value: ()) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<()> for UnitCellSize {
    fn into(self) -> () {
        self.0
    }
}

impl Control for UnitCellSize {
    type T = ();
}

/// The camera sensor pixel array readable area vertical and horizontal
/// sizes, in pixels.
///
/// The PixelArraySize property defines the size in pixel units of the
/// readable part of full pixel array matrix, including optical black
/// pixels used for calibration, pixels which are not considered valid for
/// capture and active pixels containing valid image data.
///
/// The property describes the maximum size of the raw data captured by the
/// camera, which might not correspond to the physical size of the sensor
/// pixel array matrix, as some portions of the physical pixel array matrix
/// are not accessible and cannot be transmitted out.
///
/// For example, let's consider a pixel array matrix assembled as follows
///
///      +--------------------------------------------------+
///      |xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx|
///      |xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx|
///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
///      ...          ...           ...      ...          ...
///
///      ...          ...           ...      ...          ...
///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
///      |xxDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDxx|
///      |xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx|
///      |xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx|
///      +--------------------------------------------------+
///
/// starting with two lines of non-readable pixels (x), followed by N lines
/// of readable data (D) surrounded by two columns of non-readable pixels on
/// each side, and ending with two more lines of non-readable pixels. Only
/// the readable portion is transmitted to the receiving side, defining the
/// sizes of the largest possible buffer of raw data that can be presented
/// to applications.
///
///                      PixelArraySize.width
///        /----------------------------------------------/
///        +----------------------------------------------+ /
///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| |
///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| |
///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| |
///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| |
///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| |
///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| | PixelArraySize.height
///        ...        ...           ...      ...        ...
///        ...        ...           ...      ...        ...
///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| |
///        |DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD| |
///        +----------------------------------------------+ /
///
/// This defines a rectangle whose top-left corner is placed in position (0,
/// 0) and whose vertical and horizontal sizes are defined by this property.
/// All other rectangles that describe portions of the pixel array, such as
/// the optical black pixels rectangles and active pixel areas, are defined
/// relatively to this rectangle.
///
/// All the coordinates are expressed relative to the default sensor readout
/// direction, without any transformation (such as horizontal and vertical
/// flipping) applied. When mapping them to the raw pixel buffer,
/// applications shall take any configured transformation into account.
///
/// \todo Rename this property to Size once we will have property
///       categories (i.e. Properties::PixelArray::Size)
pub struct PixelArraySize(pub ());

impl TryFrom<()> for PixelArraySize {
    type Error = Infallible;

    fn try_from(value: ()) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<()> for PixelArraySize {
    fn into(self) -> () {
        self.0
    }
}

impl Control for PixelArraySize {
    type T = ();
}

/// The pixel array region(s) which contain optical black pixels
/// considered valid for calibration purposes.
///
/// This property describes the position and size of optical black pixel
/// regions in the raw data buffer as stored in memory, which might differ
/// from their actual physical location in the pixel array matrix.
///
/// It is important to note, in fact, that camera sensors might
/// automatically reorder or skip portions of their pixels array matrix when
/// transmitting data to the receiver. For instance, a sensor may merge the
/// top and bottom optical black rectangles into a single rectangle,
/// transmitted at the beginning of the frame.
///
/// The pixel array contains several areas with different purposes,
/// interleaved by lines and columns which are said not to be valid for
/// capturing purposes. Invalid lines and columns are defined as invalid as
/// they could be positioned too close to the chip margins or to the optical
/// black shielding placed on top of optical black pixels.
///
///                      PixelArraySize.width
///        /----------------------------------------------/
///           x1                                       x2
///        +--o---------------------------------------o---+ /
///        |IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII| |
///        |IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII| |
///     y1 oIIOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOII| |
///        |IIOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOII| |
///        |IIOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOII| |
///     y2 oIIOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOII| |
///        |IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII| |
///        |IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII| |
///     y3 |IIOOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOOII| |
///        |IIOOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOOII| | PixelArraySize.height
///        |IIOOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOOII| |
///        ...          ...           ...     ...       ...
///        ...          ...           ...     ...       ...
///     y4 |IIOOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOOII| |
///        |IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII| |
///        |IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII| |
///        +----------------------------------------------+ /
///
/// The readable pixel array matrix is composed by
/// 2 invalid lines (I)
/// 4 lines of valid optical black pixels (O)
/// 2 invalid lines (I)
/// n lines of valid pixel data (P)
/// 2 invalid lines (I)
///
/// And the position of the optical black pixel rectangles is defined by
///
///     PixelArrayOpticalBlackRectangles = {
///        { x1, y1, x2 - x1 + 1, y2 - y1 + 1 },
///        { x1, y3, 2, y4 - y3 + 1 },
///        { x2, y3, 2, y4 - y3 + 1 },
///     };
///
/// If the camera, when capturing the full pixel array matrix, automatically
/// skips the invalid lines and columns, producing the following data
/// buffer, when captured to memory
///
///                      PixelArraySize.width
///        /----------------------------------------------/
///                                                    x1
///        +--------------------------------------------o-+ /
///        |OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO| |
///        |OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO| |
///        |OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO| |
///        |OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO| |
///     y1 oOOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOO| |
///        |OOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOO| |
///        |OOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOO| | PixelArraySize.height
///        ...       ...          ...       ...         ... |
///        ...       ...          ...       ...         ... |
///        |OOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOO| |
///        |OOPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPPOO| |
///        +----------------------------------------------+ /
///
/// then the invalid lines and columns should not be reported as part of the
/// PixelArraySize property in first place.
///
/// In this case, the position of the black pixel rectangles will be
///
///     PixelArrayOpticalBlackRectangles = {
///        { 0, 0, y1 + 1, PixelArraySize[0] },
///        { 0, y1, 2, PixelArraySize[1] - y1 + 1 },
///        { x1, y1, 2, PixelArraySize[1] - y1 + 1 },
///     };
///
/// \todo Rename this property to Size once we will have property
///       categories (i.e. Properties::PixelArray::OpticalBlackRectangles)
pub struct PixelArrayOpticalBlackRectangles(pub ());

impl TryFrom<()> for PixelArrayOpticalBlackRectangles {
    type Error = Infallible;

    fn try_from(value: ()) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<()> for PixelArrayOpticalBlackRectangles {
    fn into(self) -> () {
        self.0
    }
}

impl Control for PixelArrayOpticalBlackRectangles {
    type T = ();
}

/// The PixelArrayActiveAreas property defines the (possibly multiple and
/// overlapping) portions of the camera sensor readable pixel matrix
/// which are considered valid for image acquisition purposes.
///
/// This property describes an arbitrary number of overlapping rectangles,
/// with each rectangle representing the maximum image size that the camera
/// sensor can produce for a particular aspect ratio. They are defined
/// relatively to the PixelArraySize rectangle.
///
/// When multiple rectangles are reported, they shall be ordered from the
/// tallest to the shortest.
///
/// Example 1
/// A camera sensor which only produces images in the 4:3 image resolution
/// will report a single PixelArrayActiveAreas rectangle, from which all
/// other image formats are obtained by either cropping the field-of-view
/// and/or applying pixel sub-sampling techniques such as pixel skipping or
/// binning.
///
///            PixelArraySize.width
///             /----------------/
///               x1          x2
///     (0,0)-> +-o------------o-+  /
///          y1 o +------------+ |  |
///             | |////////////| |  |
///             | |////////////| |  | PixelArraySize.height
///             | |////////////| |  |
///          y2 o +------------+ |  |
///             +----------------+  /
///
/// The property reports a single rectangle
///
///          PixelArrayActiveAreas = (x1, y1, x2 - x1 + 1, y2 - y1 + 1)
///
/// Example 2
/// A camera sensor which can produce images in different native
/// resolutions will report several overlapping rectangles, one for each
/// natively supported resolution.
///
///              PixelArraySize.width
///             /------------------/
///               x1  x2    x3  x4
///     (0,0)-> +o---o------o---o+  /
///          y1 o    +------+    |  |
///             |    |//////|    |  |
///          y2 o+---+------+---+|  |
///             ||///|//////|///||  | PixelArraySize.height
///          y3 o+---+------+---+|  |
///             |    |//////|    |  |
///          y4 o    +------+    |  |
///             +----+------+----+  /
///
/// The property reports two rectangles
///
///         PixelArrayActiveAreas = ((x2, y1, x3 - x2 + 1, y4 - y1 + 1),
///                                  (x1, y2, x4 - x1 + 1, y3 - y2 + 1))
///
/// The first rectangle describes the maximum field-of-view of all image
/// formats in the 4:3 resolutions, while the second one describes the
/// maximum field of view for all image formats in the 16:9 resolutions.
///
/// Multiple rectangles shall only be reported when the sensor can't capture
/// the pixels in the corner regions. If all the pixels in the (x1,y1) -
/// (x4,y4) area can be captured, the PixelArrayActiveAreas property shall
/// contains the single rectangle (x1,y1) - (x4,y4).
///
/// \todo Rename this property to ActiveAreas once we will have property
///       categories (i.e. Properties::PixelArray::ActiveAreas)
pub struct PixelArrayActiveAreas(pub ());

impl TryFrom<()> for PixelArrayActiveAreas {
    type Error = Infallible;

    fn try_from(value: ()) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<()> for PixelArrayActiveAreas {
    fn into(self) -> () {
        self.0
    }
}

impl Control for PixelArrayActiveAreas {
    type T = ();
}

/// The maximum valid rectangle for the controls::ScalerCrop control. This
/// reflects the minimum mandatory cropping applied in the camera sensor and
/// the rest of the pipeline. Just as the ScalerCrop control, it defines a
/// rectangle taken from the sensor's active pixel array.
///
/// This property is valid only after the camera has been successfully
/// configured and its value may change whenever a new configuration is
/// applied.
///
/// \todo Turn this property into a "maximum control value" for the
/// ScalerCrop control once "dynamic" controls have been implemented.
pub struct ScalerCropMaximum(pub ());

impl TryFrom<()> for ScalerCropMaximum {
    type Error = Infallible;

    fn try_from(value: ()) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<()> for ScalerCropMaximum {
    fn into(self) -> () {
        self.0
    }
}

impl Control for ScalerCropMaximum {
    type T = ();
}

/// The relative sensitivity of the chosen sensor mode.
///
/// Some sensors have readout modes with different sensitivities. For example,
/// a binned camera mode might, with the same exposure and gains, produce
/// twice the signal level of the full resolution readout. This would be
/// signalled by the binned mode, when it is chosen, indicating a value here
/// that is twice that of the full resolution mode. This value will be valid
/// after the configure method has returned successfully.
pub struct SensorSensitivity(pub f32);

impl TryFrom<f32> for SensorSensitivity {
    type Error = Infallible;

    fn try_from(value: f32) -> Result<Self, Self::Error> {
        Ok(Self(value))
    }
}

impl Into<f32> for SensorSensitivity {
    fn into(self) -> f32 {
        self.0
    }
}

impl Control for SensorSensitivity {
    type T = f32;
}

/// The arrangement of color filters on sensor; represents the colors in the
/// top-left 2x2 section of the sensor, in reading order. Currently
/// identical to ANDROID_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT.
pub enum ColorFilterArrangement {
    /// RGGB Bayer pattern
    RGGB = 0,
    /// GRBG Bayer pattern
    GRBG = 1,
    /// GBRG Bayer pattern
    GBRG = 2,
    /// BGGR Bayer pattern
    BGGR = 3,
    /// Sensor is not Bayer; output has 3 16-bit values for each pixel,
    /// instead of just 1 16-bit value per pixel.
    RGB = 4,
    /// Sensor is not Bayer; output consists of a single colour channel.
    MONO = 5,
}

impl TryFrom<i32> for ColorFilterArrangement {
    type Error = ();

    fn try_from(value: i32) -> Result<Self, Self::Error> {
        match value {
            0 => Ok(Self::RGGB),
            1 => Ok(Self::GRBG),
            2 => Ok(Self::GBRG),
            3 => Ok(Self::BGGR),
            4 => Ok(Self::RGB),
            5 => Ok(Self::MONO),
            _ => Err(()),
        }
    }
}

impl Into<i32> for ColorFilterArrangement {
    fn into(self) -> i32 {
        self as _
    }
}

impl Control for ColorFilterArrangement {
    type T = i32;
}
